{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TQS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "MAX_LENGTH = 100\n",
    "NUM_HEADS = 1\n",
    "NUM_LAYERS = 1\n",
    "DIM_FEEDFORWARD = 128\n",
    "TEST_LENGTH = 50\n",
    "TEST_BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_potentials = torch.randn(TEST_LENGTH, TEST_BATCH)\n",
    "test_spins = torch.zeros(TEST_LENGTH, TEST_BATCH)\n",
    "test_spins[torch.randint(0, TEST_LENGTH, (TEST_BATCH,)), torch.arange(TEST_BATCH)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "tqs = TQS(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    max_chain_len=MAX_LENGTH,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    possible_spins=2,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, phases = tqs(test_potentials, test_spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3880, 0.6342],\n",
       "         [0.3739, 0.5909],\n",
       "         [0.3262, 0.1927],\n",
       "         ...,\n",
       "         [0.2705, 0.2694],\n",
       "         [0.4124, 0.2042],\n",
       "         [0.3862, 0.1753]],\n",
       "\n",
       "        [[0.2565, 0.3889],\n",
       "         [0.2719, 0.2789],\n",
       "         [0.2870, 0.5833],\n",
       "         ...,\n",
       "         [0.2133, 0.3178],\n",
       "         [0.4191, 0.2030],\n",
       "         [0.2331, 0.3933]],\n",
       "\n",
       "        [[0.2866, 0.5047],\n",
       "         [0.2910, 0.5431],\n",
       "         [0.3412, 0.5623],\n",
       "         ...,\n",
       "         [0.3558, 0.6297],\n",
       "         [0.2745, 0.4499],\n",
       "         [0.2557, 0.4731]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5138, 0.6078],\n",
       "         [0.5418, 0.6153],\n",
       "         [0.5583, 0.6089],\n",
       "         ...,\n",
       "         [0.5002, 0.5850],\n",
       "         [0.5501, 0.6191],\n",
       "         [0.5615, 0.5280]],\n",
       "\n",
       "        [[0.5419, 0.5915],\n",
       "         [0.5416, 0.6317],\n",
       "         [0.5403, 0.6200],\n",
       "         ...,\n",
       "         [0.5285, 0.6102],\n",
       "         [0.5311, 0.6396],\n",
       "         [0.5144, 0.6207]],\n",
       "\n",
       "        [[0.5273, 0.7059],\n",
       "         [0.5227, 0.6584],\n",
       "         [0.5212, 0.6967],\n",
       "         ...,\n",
       "         [0.5179, 0.6732],\n",
       "         [0.5074, 0.6723],\n",
       "         [0.5363, 0.6829]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def autoregressive_sample(model, initial_potentials, max_length):\n",
    "    model.eval()\n",
    "    # Allocate a buffer for the sampled basis states\n",
    "    sampled_spins = torch.zeros(max_length, initial_potentials.size(1))\n",
    "    batch_size = initial_potentials.size(1)\n",
    "    batch_remaining_idx = torch.arange(batch_size)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # get P(s_{i+1} | V, s_{1:i}) and phi(s_{i+1} | V, s_{1:i}) distributions\n",
    "        probs, _ = model(\n",
    "            initial_potentials[:, batch_remaining_idx],\n",
    "            sampled_spins[:i, batch_remaining_idx],\n",
    "        )\n",
    "\n",
    "        # sample s_{i+1} from P(s_{i+1} | V, s_{1:i})\n",
    "        last_probs = probs[-1]  # (batch, 2); P(s_{i+1} | V, s_{1:i})\n",
    "        sampled_spins[i, batch_remaining_idx] = (\n",
    "            torch.multinomial(last_probs, 1).squeeze().float()\n",
    "        )\n",
    "\n",
    "        # a mask with dimension (batch_remaining,); True if we sampled a 1\n",
    "        newly_completed = sampled_spins[i, batch_remaining_idx] == 1.0\n",
    "\n",
    "        # mask out the batch_remaining_idx that have been completed\n",
    "        batch_remaining_idx = batch_remaining_idx[~newly_completed]\n",
    "\n",
    "    return sampled_spins\n",
    "\n",
    "\n",
    "sampled_spins = autoregressive_sample(tqs, test_potentials, TEST_LENGTH)\n",
    "sampled_spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_spins.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_spins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_spins = tqs.sample_spins(test_potentials, TEST_LENGTH)\n",
    "sampled_spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    sampled_spins.sum(dim=0) == 1.0\n",
    ").all()  # each batch should have exactly one 1 spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "T = 1.0\n",
    "print(tqs.compute_psi(sampled_spins, test_potentials, T).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 8.6045e-01-0.2001j,  4.7735e-01+0.1902j,  8.6123e-01-0.1947j,\n",
       "           ...,  8.5597e-01-0.2224j,  8.2973e-01-0.3186j,\n",
       "           8.5885e-01-0.2116j],\n",
       "         [ 4.7721e-01-0.2207j,  4.4824e-01-0.3046j,  4.7875e-01-0.2148j,\n",
       "           ...,  4.8823e-01-0.1710j,  4.3818e-01-0.3268j,\n",
       "           4.7186e-01-0.2397j],\n",
       "         [ 4.2254e-01-0.3376j,  6.4744e-01-0.6134j,  4.2621e-01-0.3316j,\n",
       "           ...,  4.5389e-01-0.2752j,  3.5103e-01-0.4388j,\n",
       "           4.1185e-01-0.3547j],\n",
       "         ...,\n",
       "         [ 2.7540e-01+0.7052j,  3.7484e-01+0.6695j,  2.6165e-01+0.7087j,\n",
       "           ...,  2.1205e-01+0.7190j,  4.1202e-01+0.6511j,\n",
       "           2.9132e-01+0.7007j],\n",
       "         [-1.3793e-01+0.7691j, -4.6143e-02+0.7894j, -1.4981e-01+0.7655j,\n",
       "           ..., -1.8211e-01+0.7545j, -5.2191e-04+0.7942j,\n",
       "          -1.2066e-01+0.7743j],\n",
       "         [-3.8453e-01+0.7177j, -3.1498e-01+0.7600j, -3.9147e-01+0.7128j,\n",
       "           ..., -4.0423e-01+0.7029j, -2.7110e-01+0.7805j,\n",
       "          -3.7147e-01+0.7270j]], grad_fn=<MulBackward0>),\n",
       " tensor([[ 0.4230+0.2628j,  0.4783+0.1882j,  0.4198+0.2651j,  ...,\n",
       "           0.4070+0.2793j,  0.4966+0.1556j,  0.4377+0.2468j],\n",
       "         [ 0.7273-0.5089j,  0.4480-0.3049j,  0.7267-0.5093j,  ...,\n",
       "           0.7177-0.5236j,  0.6631-0.5982j,  0.7221-0.5175j],\n",
       "         [ 0.4221-0.3384j,  0.3645-0.4219j,  0.4266-0.3312j,  ...,\n",
       "           0.4520-0.2800j,  0.3484-0.4418j,  0.4124-0.3541j],\n",
       "         ...,\n",
       "         [ 0.2732+0.7059j,  0.3734+0.6700j,  0.2615+0.7088j,  ...,\n",
       "           0.2173+0.7182j,  0.4103+0.6522j,  0.2886+0.7016j],\n",
       "         [-0.1406+0.7685j, -0.0462+0.7892j, -0.1491+0.7656j,  ...,\n",
       "          -0.1752+0.7570j, -0.0051+0.7941j, -0.1232+0.7737j],\n",
       "         [-0.3867+0.7164j, -0.3136+0.7606j, -0.3905+0.7133j,  ...,\n",
       "          -0.3986+0.7071j, -0.2769+0.7782j, -0.3727+0.7262j]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[ 0.4259+0.2593j,  0.4781+0.1889j,  0.4183+0.2678j,  ...,\n",
       "           0.3995+0.2873j,  0.4966+0.1546j,  0.4365+0.2487j],\n",
       "         [ 0.4774-0.2213j,  0.6762-0.5821j,  0.4796-0.2127j,  ...,\n",
       "           0.4896-0.1647j,  0.4388-0.3263j,  0.4730-0.2370j],\n",
       "         [ 0.4231-0.3372j,  0.3634-0.4233j,  0.4269-0.3306j,  ...,\n",
       "           0.4569-0.2685j,  0.3527-0.4375j,  0.4134-0.3528j],\n",
       "         ...,\n",
       "         [ 0.2775+0.7048j,  0.3787+0.6678j,  0.2628+0.7088j,  ...,\n",
       "           0.2089+0.7198j,  0.4145+0.6501j,  0.2893+0.7015j],\n",
       "         [-0.1353+0.7700j, -0.0416+0.7900j, -0.1500+0.7658j,  ...,\n",
       "          -0.1861+0.7533j,  0.0037+0.7946j, -0.1229+0.7738j],\n",
       "         [ 0.6726+0.6559j, -0.3114+0.7618j,  0.6604+0.6677j,  ...,\n",
       "           0.6776+0.6511j,  0.7594+0.5589j,  0.6741+0.6551j]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x, psi_l, psi_r = tqs.psi_terms(sampled_spins, test_potentials, T)\n",
    "assert (\n",
    "    psi_x.shape == psi_l.shape == psi_r.shape == torch.Size([TEST_LENGTH, TEST_BATCH])\n",
    ")\n",
    "psi_x, psi_l, psi_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0252-0.5088j, -2.5745-0.2215j, -0.2366-0.9237j,  ...,\n",
       "         -0.8084-0.8268j, -0.1522-1.0205j, -0.2360-0.9256j],\n",
       "        [-2.7907+0.3581j, -2.5697+0.1430j, -3.0114+0.4719j,  ...,\n",
       "         -2.6933+0.5022j, -2.0626-0.2703j, -2.7904+0.3153j],\n",
       "        [-2.2532+0.2022j, -1.7075+0.5655j, -2.4258+0.3280j,  ...,\n",
       "         -2.5045+0.3018j, -2.0796+0.1011j, -2.1521+0.1252j],\n",
       "        ...,\n",
       "        [-2.0713-0.1817j, -1.8650+0.2448j, -2.0019-0.0023j,  ...,\n",
       "         -1.9699+0.1073j, -1.7641+0.3744j, -2.6508-1.5726j],\n",
       "        [-1.8399-0.8943j, -1.9672-0.5611j, -1.9661-0.1751j,  ...,\n",
       "         -1.6152-1.5932j, -2.0006+0.4676j, -1.9523-0.3093j],\n",
       "        [-1.1517+0.7919j, -2.2078+0.5073j, -0.9665+0.4490j,  ...,\n",
       "         -1.1032+0.8251j, -1.1559+0.5609j, -1.1045+0.6404j]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_loc = tqs.E_loc(psi_x, psi_l, psi_r, test_potentials, T)\n",
    "E_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6045e-01-0.2001j,  4.7735e-01+0.1902j,  8.6123e-01-0.1947j,\n",
       "          ...,  8.5597e-01-0.2224j,  8.2973e-01-0.3186j,\n",
       "          8.5885e-01-0.2116j],\n",
       "        [ 4.7721e-01-0.2207j,  4.4824e-01-0.3046j,  4.7875e-01-0.2148j,\n",
       "          ...,  4.8823e-01-0.1710j,  4.3818e-01-0.3268j,\n",
       "          4.7186e-01-0.2397j],\n",
       "        [ 4.2254e-01-0.3376j,  6.4744e-01-0.6134j,  4.2621e-01-0.3316j,\n",
       "          ...,  4.5389e-01-0.2752j,  3.5103e-01-0.4388j,\n",
       "          4.1185e-01-0.3547j],\n",
       "        ...,\n",
       "        [ 2.7540e-01+0.7052j,  3.7484e-01+0.6695j,  2.6165e-01+0.7087j,\n",
       "          ...,  2.1205e-01+0.7190j,  4.1202e-01+0.6511j,\n",
       "          2.9132e-01+0.7007j],\n",
       "        [-1.3793e-01+0.7691j, -4.6143e-02+0.7894j, -1.4981e-01+0.7655j,\n",
       "          ..., -1.8211e-01+0.7545j, -5.2191e-04+0.7942j,\n",
       "         -1.2066e-01+0.7743j],\n",
       "        [-3.8453e-01+0.7177j, -3.1498e-01+0.7600j, -3.9147e-01+0.7128j,\n",
       "          ..., -4.0423e-01+0.7029j, -2.7110e-01+0.7805j,\n",
       "         -3.7147e-01+0.7270j]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-9.0678e-01, -3.4904e-01, -5.8869e-01, -5.8170e-02, -4.1572e-01,\n",
      "         3.4841e-01,  8.9565e-01,  8.1810e-01, -3.4364e-01,  2.9650e-01,\n",
      "        -2.8507e-01,  1.6111e-03, -1.2105e+00, -1.5277e+00, -8.0490e-01,\n",
      "         1.0923e+00, -4.3110e-01,  1.2369e+00, -5.4244e-01,  1.2190e+00,\n",
      "        -1.4323e+00, -2.0283e-01,  9.3111e-01, -3.9909e-01,  1.4032e+00,\n",
      "         1.0671e+00, -1.0788e-01,  1.7218e-01,  4.4500e-01, -9.8702e-01,\n",
      "         1.4303e+00,  3.1006e-02,  5.0137e-01,  5.9805e-01, -3.8208e-01,\n",
      "        -1.9614e+00, -1.5861e+00,  2.2086e-03,  1.3585e+00,  8.5802e-01,\n",
      "        -1.9340e-01,  2.1199e-01,  2.2446e-01, -1.6655e+00,  3.4554e-01,\n",
      "        -1.0952e+00, -8.4067e-01, -8.3924e-01, -4.1254e-01,  1.0622e+00,\n",
      "         5.1169e-01, -1.8868e+00,  2.0948e+00,  4.1218e-01, -1.1691e+00,\n",
      "         8.3573e-01,  9.1111e-01, -2.5913e-01,  3.3681e-01, -6.5043e-01,\n",
      "         4.2778e-01, -4.1978e-01, -9.3214e-02,  5.1071e-01],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.0621e-01, -4.1576e-01,  1.5720e+00, -1.9628e+00,  1.0156e-01,\n",
      "        -8.5258e-01,  1.2410e-03,  2.4853e-01,  8.4825e-01,  2.2581e-01,\n",
      "         1.1875e+00,  4.4755e-01,  1.6094e+00,  3.8813e-01,  1.6512e+00,\n",
      "         8.4545e-01, -6.0814e-01, -3.5535e-01, -1.0162e+00, -2.9925e-02,\n",
      "        -1.0524e+00,  7.5516e-02,  2.8595e-01,  6.0306e-02, -1.5021e+00,\n",
      "         7.9424e-01,  1.0566e+00, -1.4693e+00,  6.2027e-01,  1.3335e+00,\n",
      "         3.2648e-01, -1.4020e+00,  1.3325e+00,  6.8776e-02,  8.2781e-02,\n",
      "        -1.2740e-01, -1.1705e-01,  1.9967e+00, -4.9932e-01, -1.1324e+00,\n",
      "        -2.0526e+00, -7.3310e-01,  6.3021e-01,  4.4426e-01,  1.3370e+00,\n",
      "         2.2126e+00, -1.1488e+00, -7.3158e-01, -1.7910e+00, -1.6005e+00,\n",
      "        -3.2128e-01,  2.9681e+00, -4.4710e-01,  1.3422e+00,  8.9315e-02,\n",
      "        -1.8896e-02, -6.6344e-01,  9.8839e-01,  1.7385e+00, -8.1939e-01,\n",
      "        -8.9971e-01, -7.3625e-01, -1.2428e+00, -8.4137e-01],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2801e-01, -1.1145e-01,  1.4450e-01,  ..., -1.5119e-01,\n",
      "          4.2330e-02,  1.6464e-02],\n",
      "        [-6.9296e-05, -1.0884e-01, -5.6226e-02,  ..., -9.5766e-02,\n",
      "          5.8523e-02,  1.0551e-01],\n",
      "        [ 1.8708e-02,  3.2815e-03, -9.5432e-02,  ...,  1.4725e-01,\n",
      "         -9.7539e-02, -8.2763e-03],\n",
      "        ...,\n",
      "        [-1.4937e-01, -6.3097e-03, -7.1917e-02,  ...,  1.4284e-01,\n",
      "          7.9292e-03,  1.0689e-01],\n",
      "        [-6.2650e-02,  6.1587e-02, -6.1612e-02,  ...,  3.7246e-03,\n",
      "          4.2131e-02,  4.2971e-02],\n",
      "        [-2.8071e-02,  7.0940e-02,  4.0699e-02,  ..., -1.2049e-01,\n",
      "         -1.4896e-01,  4.3561e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1227,  0.0389,  0.0777,  ..., -0.0883,  0.1116,  0.0609],\n",
      "        [ 0.0622,  0.0873,  0.0687,  ..., -0.0956,  0.1036,  0.0227],\n",
      "        [ 0.0803, -0.0595, -0.0511,  ..., -0.1059,  0.1109, -0.0889],\n",
      "        ...,\n",
      "        [-0.0615, -0.0267, -0.0104,  ..., -0.1024, -0.0720,  0.0061],\n",
      "        [-0.0433,  0.1183, -0.1205,  ...,  0.0428, -0.0830, -0.0157],\n",
      "        [-0.0308,  0.1220,  0.1117,  ...,  0.0500,  0.0965, -0.1008]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0132,  0.0061, -0.0809,  ..., -0.0636,  0.0361,  0.1047],\n",
      "        [-0.1001,  0.1060, -0.0636,  ...,  0.0903, -0.0300,  0.0687],\n",
      "        [ 0.0273, -0.0644,  0.0412,  ..., -0.1093, -0.0457, -0.0551],\n",
      "        ...,\n",
      "        [-0.1148, -0.0591, -0.0125,  ..., -0.0822,  0.0228, -0.0900],\n",
      "        [ 0.0582,  0.0444, -0.1025,  ..., -0.0542,  0.0490, -0.1153],\n",
      "        [ 0.0172, -0.0005, -0.1192,  ...,  0.1238, -0.0324,  0.0590]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1022, -0.0907,  0.0516,  0.1209, -0.0520, -0.0977,  0.0203, -0.0526,\n",
      "        -0.0879,  0.0164,  0.0204,  0.0700,  0.0805, -0.1170, -0.0453, -0.0952,\n",
      "        -0.0606,  0.0146,  0.0359, -0.0777, -0.0853, -0.1092,  0.0251,  0.1158,\n",
      "        -0.0008,  0.0663,  0.0789,  0.1140,  0.1212,  0.0082, -0.0559,  0.0201,\n",
      "         0.0640,  0.0403, -0.1207, -0.0825,  0.0075, -0.0005, -0.0209,  0.0429,\n",
      "        -0.1143, -0.0835,  0.1216,  0.0495,  0.0787,  0.0032,  0.0024, -0.0039,\n",
      "        -0.0396, -0.1006,  0.0153,  0.0385, -0.0263, -0.1068, -0.0145,  0.0947,\n",
      "        -0.0933, -0.0963,  0.0338, -0.0239,  0.0587, -0.1032, -0.0497, -0.0079,\n",
      "        -0.0585, -0.0100, -0.1057,  0.0757,  0.0179,  0.0926,  0.0136, -0.0263,\n",
      "        -0.1059,  0.1015,  0.0186,  0.0922,  0.0162, -0.1213, -0.1201, -0.0665,\n",
      "         0.0811, -0.0143,  0.1129, -0.0422, -0.0372,  0.0831, -0.1018,  0.0107,\n",
      "        -0.0023, -0.0808,  0.0475, -0.0168,  0.0880, -0.0417, -0.0753,  0.0737,\n",
      "         0.1039, -0.0698, -0.0674, -0.0296,  0.0723, -0.1093,  0.0814, -0.1196,\n",
      "         0.0901, -0.0112, -0.0603, -0.0990,  0.0343,  0.1144, -0.0584,  0.0710,\n",
      "         0.0910, -0.0423,  0.0011,  0.0832,  0.0631,  0.0015, -0.0110,  0.0717,\n",
      "        -0.0433,  0.0701,  0.0077,  0.0999, -0.0691,  0.0370, -0.0003,  0.1240],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0763, -0.0668, -0.0815,  ..., -0.0111, -0.0488,  0.0308],\n",
      "        [-0.0689, -0.0149, -0.0619,  ..., -0.0004,  0.0846,  0.0297],\n",
      "        [ 0.0176, -0.0879, -0.0650,  ..., -0.0038,  0.0414, -0.0579],\n",
      "        ...,\n",
      "        [ 0.0870,  0.0654,  0.0851,  ..., -0.0588, -0.0436,  0.0425],\n",
      "        [ 0.0294, -0.0594, -0.0091,  ..., -0.0836, -0.0303, -0.0476],\n",
      "        [ 0.0641,  0.0880,  0.0166,  ...,  0.0520, -0.0046,  0.0798]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0044,  0.0803, -0.0042, -0.0864, -0.0218, -0.0880,  0.0097, -0.0561,\n",
      "        -0.0836,  0.0487,  0.0390, -0.0265,  0.0153,  0.0724,  0.0295, -0.0312,\n",
      "         0.0058, -0.0715,  0.0269,  0.0745, -0.0481, -0.0002, -0.0712, -0.0003,\n",
      "        -0.0272, -0.0186, -0.0363, -0.0216,  0.0676, -0.0135,  0.0416, -0.0599,\n",
      "        -0.0042, -0.0042, -0.0746,  0.0227,  0.0382, -0.0632,  0.0530,  0.0296,\n",
      "        -0.0605, -0.0105, -0.0529,  0.0077, -0.0575, -0.0853, -0.0497,  0.0013,\n",
      "        -0.0125, -0.0182,  0.0099,  0.0284,  0.0002,  0.0414,  0.0860, -0.0697,\n",
      "         0.0798, -0.0106, -0.0086, -0.0450, -0.0100, -0.0856,  0.0650, -0.0006],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2801e-01, -1.1145e-01,  1.4450e-01,  ..., -1.5119e-01,\n",
      "          4.2330e-02,  1.6464e-02],\n",
      "        [-6.9296e-05, -1.0884e-01, -5.6226e-02,  ..., -9.5766e-02,\n",
      "          5.8523e-02,  1.0551e-01],\n",
      "        [ 1.8708e-02,  3.2815e-03, -9.5432e-02,  ...,  1.4725e-01,\n",
      "         -9.7539e-02, -8.2763e-03],\n",
      "        ...,\n",
      "        [-1.4937e-01, -6.3097e-03, -7.1917e-02,  ...,  1.4284e-01,\n",
      "          7.9292e-03,  1.0689e-01],\n",
      "        [-6.2650e-02,  6.1587e-02, -6.1612e-02,  ...,  3.7246e-03,\n",
      "          4.2131e-02,  4.2971e-02],\n",
      "        [-2.8071e-02,  7.0940e-02,  4.0699e-02,  ..., -1.2049e-01,\n",
      "         -1.4896e-01,  4.3561e-02]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1227,  0.0389,  0.0777,  ..., -0.0883,  0.1116,  0.0609],\n",
      "        [ 0.0622,  0.0873,  0.0687,  ..., -0.0956,  0.1036,  0.0227],\n",
      "        [ 0.0803, -0.0595, -0.0511,  ..., -0.1059,  0.1109, -0.0889],\n",
      "        ...,\n",
      "        [-0.0615, -0.0267, -0.0104,  ..., -0.1024, -0.0720,  0.0061],\n",
      "        [-0.0433,  0.1183, -0.1205,  ...,  0.0428, -0.0830, -0.0157],\n",
      "        [-0.0308,  0.1220,  0.1117,  ...,  0.0500,  0.0965, -0.1008]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0132,  0.0061, -0.0809,  ..., -0.0636,  0.0361,  0.1047],\n",
      "        [-0.1001,  0.1060, -0.0636,  ...,  0.0903, -0.0300,  0.0687],\n",
      "        [ 0.0273, -0.0644,  0.0412,  ..., -0.1093, -0.0457, -0.0551],\n",
      "        ...,\n",
      "        [-0.1148, -0.0591, -0.0125,  ..., -0.0822,  0.0228, -0.0900],\n",
      "        [ 0.0582,  0.0444, -0.1025,  ..., -0.0542,  0.0490, -0.1153],\n",
      "        [ 0.0172, -0.0005, -0.1192,  ...,  0.1238, -0.0324,  0.0590]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1022, -0.0907,  0.0516,  0.1209, -0.0520, -0.0977,  0.0203, -0.0526,\n",
      "        -0.0879,  0.0164,  0.0204,  0.0700,  0.0805, -0.1170, -0.0453, -0.0952,\n",
      "        -0.0606,  0.0146,  0.0359, -0.0777, -0.0853, -0.1092,  0.0251,  0.1158,\n",
      "        -0.0008,  0.0663,  0.0789,  0.1140,  0.1212,  0.0082, -0.0559,  0.0201,\n",
      "         0.0640,  0.0403, -0.1207, -0.0825,  0.0075, -0.0005, -0.0209,  0.0429,\n",
      "        -0.1143, -0.0835,  0.1216,  0.0495,  0.0787,  0.0032,  0.0024, -0.0039,\n",
      "        -0.0396, -0.1006,  0.0153,  0.0385, -0.0263, -0.1068, -0.0145,  0.0947,\n",
      "        -0.0933, -0.0963,  0.0338, -0.0239,  0.0587, -0.1032, -0.0497, -0.0079,\n",
      "        -0.0585, -0.0100, -0.1057,  0.0757,  0.0179,  0.0926,  0.0136, -0.0263,\n",
      "        -0.1059,  0.1015,  0.0186,  0.0922,  0.0162, -0.1213, -0.1201, -0.0665,\n",
      "         0.0811, -0.0143,  0.1129, -0.0422, -0.0372,  0.0831, -0.1018,  0.0107,\n",
      "        -0.0023, -0.0808,  0.0475, -0.0168,  0.0880, -0.0417, -0.0753,  0.0737,\n",
      "         0.1039, -0.0698, -0.0674, -0.0296,  0.0723, -0.1093,  0.0814, -0.1196,\n",
      "         0.0901, -0.0112, -0.0603, -0.0990,  0.0343,  0.1144, -0.0584,  0.0710,\n",
      "         0.0910, -0.0423,  0.0011,  0.0832,  0.0631,  0.0015, -0.0110,  0.0717,\n",
      "        -0.0433,  0.0701,  0.0077,  0.0999, -0.0691,  0.0370, -0.0003,  0.1240],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0763, -0.0668, -0.0815,  ..., -0.0111, -0.0488,  0.0308],\n",
      "        [-0.0689, -0.0149, -0.0619,  ..., -0.0004,  0.0846,  0.0297],\n",
      "        [ 0.0176, -0.0879, -0.0650,  ..., -0.0038,  0.0414, -0.0579],\n",
      "        ...,\n",
      "        [ 0.0870,  0.0654,  0.0851,  ..., -0.0588, -0.0436,  0.0425],\n",
      "        [ 0.0294, -0.0594, -0.0091,  ..., -0.0836, -0.0303, -0.0476],\n",
      "        [ 0.0641,  0.0880,  0.0166,  ...,  0.0520, -0.0046,  0.0798]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0044,  0.0803, -0.0042, -0.0864, -0.0218, -0.0880,  0.0097, -0.0561,\n",
      "        -0.0836,  0.0487,  0.0390, -0.0265,  0.0153,  0.0724,  0.0295, -0.0312,\n",
      "         0.0058, -0.0715,  0.0269,  0.0745, -0.0481, -0.0002, -0.0712, -0.0003,\n",
      "        -0.0272, -0.0186, -0.0363, -0.0216,  0.0676, -0.0135,  0.0416, -0.0599,\n",
      "        -0.0042, -0.0042, -0.0746,  0.0227,  0.0382, -0.0632,  0.0530,  0.0296,\n",
      "        -0.0605, -0.0105, -0.0529,  0.0077, -0.0575, -0.0853, -0.0497,  0.0013,\n",
      "        -0.0125, -0.0182,  0.0099,  0.0284,  0.0002,  0.0414,  0.0860, -0.0697,\n",
      "         0.0798, -0.0106, -0.0086, -0.0450, -0.0100, -0.0856,  0.0650, -0.0006],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0911, -0.0139, -0.0858, -0.0444, -0.0551,  0.0653, -0.0484,  0.0101,\n",
      "          0.0839, -0.1079, -0.0122, -0.1056,  0.0197, -0.1154,  0.0689,  0.0112,\n",
      "          0.0639, -0.0450,  0.0901,  0.0970,  0.0270, -0.1173,  0.0915,  0.0285,\n",
      "         -0.0160,  0.0012, -0.1044, -0.0147,  0.0192,  0.0251,  0.0744,  0.0980,\n",
      "         -0.1238,  0.0792,  0.0618,  0.1114,  0.0418,  0.0109,  0.0937, -0.1066,\n",
      "          0.1174,  0.0593,  0.0997,  0.0126, -0.0358,  0.0329,  0.0742,  0.0140,\n",
      "         -0.0293,  0.0116,  0.0312, -0.0963,  0.0362, -0.0715,  0.0173, -0.0254,\n",
      "          0.0382, -0.0899, -0.1054,  0.0169, -0.0068,  0.0946, -0.0842, -0.1174],\n",
      "        [ 0.0432,  0.0464,  0.0198,  0.0935,  0.1228, -0.0464, -0.0014, -0.1019,\n",
      "         -0.0183, -0.0733, -0.0340, -0.1096,  0.0528,  0.0480,  0.1195, -0.0957,\n",
      "         -0.0141, -0.0638,  0.0753, -0.0643,  0.0737, -0.0252,  0.0760, -0.0343,\n",
      "         -0.0587, -0.0060,  0.1118, -0.0921,  0.0175,  0.1125,  0.1003, -0.0143,\n",
      "          0.1226,  0.1149,  0.0225,  0.0720, -0.0421,  0.1181, -0.0968, -0.1199,\n",
      "         -0.0626, -0.1186,  0.1000,  0.0182,  0.0479,  0.0992, -0.0585,  0.1014,\n",
      "         -0.0519, -0.0451,  0.1066,  0.0703,  0.0294,  0.0702, -0.1021, -0.0111,\n",
      "          0.1186,  0.0721,  0.0647, -0.0054, -0.0781, -0.1158,  0.0088,  0.0423]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1179, 0.0882], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0958,  0.1147, -0.0558, -0.1215, -0.0190, -0.1137, -0.0546, -0.0996,\n",
      "          0.0127,  0.0900, -0.0498,  0.1189,  0.0651, -0.1128,  0.0022, -0.1118,\n",
      "         -0.0701, -0.1237,  0.1016,  0.0983, -0.0386,  0.0470, -0.0914, -0.0402,\n",
      "          0.0975,  0.0120,  0.1191,  0.0237, -0.0622, -0.0559, -0.0192,  0.0848,\n",
      "          0.0800, -0.0985, -0.0160,  0.0201,  0.0364, -0.0045,  0.0753,  0.1001,\n",
      "         -0.0130,  0.0998, -0.0736,  0.0033,  0.1230,  0.0560, -0.0966, -0.0286,\n",
      "          0.1211,  0.0960,  0.0183, -0.0276, -0.0403,  0.0868,  0.0456,  0.0753,\n",
      "          0.1080, -0.0219,  0.0957,  0.1039, -0.0737,  0.0226, -0.0385, -0.0503]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0945], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in tqs.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6045e-01-0.2001j,  4.7735e-01+0.1902j,  8.6123e-01-0.1947j,\n",
       "          ...,  8.5597e-01-0.2224j,  8.2973e-01-0.3186j,\n",
       "          8.5885e-01-0.2116j],\n",
       "        [ 4.7721e-01-0.2207j,  4.4824e-01-0.3046j,  4.7875e-01-0.2148j,\n",
       "          ...,  4.8823e-01-0.1710j,  4.3818e-01-0.3268j,\n",
       "          4.7186e-01-0.2397j],\n",
       "        [ 4.2254e-01-0.3376j,  6.4744e-01-0.6134j,  4.2621e-01-0.3316j,\n",
       "          ...,  4.5389e-01-0.2752j,  3.5103e-01-0.4388j,\n",
       "          4.1185e-01-0.3547j],\n",
       "        ...,\n",
       "        [ 2.7540e-01+0.7052j,  3.7484e-01+0.6695j,  2.6165e-01+0.7087j,\n",
       "          ...,  2.1205e-01+0.7190j,  4.1202e-01+0.6511j,\n",
       "          2.9132e-01+0.7007j],\n",
       "        [-1.3793e-01+0.7691j, -4.6143e-02+0.7894j, -1.4981e-01+0.7655j,\n",
       "          ..., -1.8211e-01+0.7545j, -5.2191e-04+0.7942j,\n",
       "         -1.2066e-01+0.7743j],\n",
       "        [-3.8453e-01+0.7177j, -3.1498e-01+0.7600j, -3.9147e-01+0.7128j,\n",
       "          ..., -4.0423e-01+0.7029j, -2.7110e-01+0.7805j,\n",
       "         -3.7147e-01+0.7270j]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/example-for-one-of-the-differentiated-tensors-appears-to-not-have-been-used-in-the-graph/58396\n",
    "a = torch.rand(10, requires_grad=True)\n",
    "b = torch.rand(10, requires_grad=True)\n",
    "\n",
    "output = (2 * a).sum()\n",
    "\n",
    "torch.autograd.grad(output, (a), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n",
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "print(test_potentials.shape)\n",
    "print(sampled_spins.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/functional.py:5504: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_flash_attention_for_cpu. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1718580740865/work/aten/src/ATen/functorch/BatchedFallback.cpp:81.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 3 dims. Maybe you wanted to use the vjp or jacrev APIs instead?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(dlnP_dTheta \u001b[38;5;241m:=\u001b[39m tqs\u001b[38;5;241m.\u001b[39mdlnP_dTheta(sampled_spins, test_potentials))\n",
      "File \u001b[0;32m~/Projects/tqs-localization/model.py:158\u001b[0m, in \u001b[0;36mTQS.dlnP_dTheta\u001b[0;34m(self, x, V)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# in_dims=(1, 1, None, None) produces iteration over dimension 1 (the batch dim) of the\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# spins and the potentials and does no \"ein-iteration\" over the parameters or buffers\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# (treated as constant with respect to the iteration)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m vmap_dlnP_dtheta \u001b[38;5;241m=\u001b[39m vmap(dlnP_dtheta_sample, in_dims\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vmap_dlnP_dtheta(x, V, params, buffers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    282\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:363\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m eager_transforms\u001b[38;5;241m.\u001b[39mgrad_impl(func, argnums, has_aux, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1285\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[0;32m-> 1285\u001b[0m     results \u001b[38;5;241m=\u001b[39m grad_and_value_impl(func, argnums, has_aux, args, kwargs)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1287\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1262\u001b[0m, in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1260\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto return a Tensor, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_and_value(f)(*args): Expected f(*args) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1263\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto return a scalar Tensor, got tensor with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1264\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Maybe you wanted to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1265\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse the vjp or jacrev APIs instead?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1267\u001b[0m flat_diff_args, spec \u001b[38;5;241m=\u001b[39m tree_flatten(diff_args)\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;66;03m# NB: need create_graph so that backward pass isn't run in no_grad mode\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad_and_value(f)(*args): Expected f(*args) to return a scalar Tensor, got tensor with 3 dims. Maybe you wanted to use the vjp or jacrev APIs instead?"
     ]
    }
   ],
   "source": [
    "print(dlnP_dTheta := tqs.dlnP_dTheta(sampled_spins, test_potentials))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is `jacrev` the right function to use here and what does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, vmap, jacrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a spin chain and a corresponding potential function,\n",
    "# recalling that their dimensions are (seq, batch)\n",
    "a_spin_chain = sampled_spins.clone().detach().requires_grad_(True)[:, 1]\n",
    "a_potential_func = test_potentials.clone().detach().requires_grad_(True)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([-1.2020,  0.1472, -0.7155, -0.0764, -0.3391, -0.7191, -0.3792, -0.7279,\n",
      "         0.6413, -0.0596,  0.5746, -0.0770,  0.3573,  0.3304, -0.9190,  0.4531,\n",
      "         2.0006, -0.2597,  0.3847,  0.9929,  2.4995, -0.9985,  1.0245, -1.8078,\n",
      "        -0.1615,  0.7566,  1.5770, -1.7874, -0.0235,  0.0087, -0.2809, -0.5713,\n",
      "         0.2140, -0.9265,  1.5526,  0.9636, -0.0928,  1.3318,  0.5748,  0.2954,\n",
      "         0.2612,  1.5846,  0.2285, -0.4883, -0.1614,  0.1229,  0.8383,  0.3604,\n",
      "        -0.7181,  0.6586], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a_spin_chain)\n",
    "print(a_potential_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 2, 50, 1])\n",
      "torch.Size([100, 1, 1, 50, 1])\n"
     ]
    }
   ],
   "source": [
    "params = {k: v.detach for k, v in tqs.named_parameters()}\n",
    "buffers = {k: v.detach for k, v in tqs.named_buffers()}\n",
    "\n",
    "deriv = jacrev(tqs, argnums=1)(a_potential_func.unsqueeze(1), a_spin_chain.unsqueeze(1))\n",
    "\n",
    "# Jacobian of probabilities; Jacobian of phases.\n",
    "for d in deriv:\n",
    "    print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Jacobian of $\\ln P(x; \\Theta)$\n",
    "- For a singleton-batch spin chain\n",
    "- Should be with the jacrev function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9709, -0.4955]],\n",
       "\n",
       "        [[-1.3604, -1.4039]],\n",
       "\n",
       "        [[-1.1900, -0.5641]],\n",
       "\n",
       "        [[-1.1640, -1.0406]],\n",
       "\n",
       "        [[-0.8777, -0.7167]],\n",
       "\n",
       "        [[-0.7257, -0.4631]],\n",
       "\n",
       "        [[-0.6685, -0.4927]],\n",
       "\n",
       "        [[-0.6746, -0.3667]],\n",
       "\n",
       "        [[-0.5620, -1.1268]],\n",
       "\n",
       "        [[-0.4962, -0.6093]],\n",
       "\n",
       "        [[-0.4236, -1.1076]],\n",
       "\n",
       "        [[-0.3556, -0.5925]],\n",
       "\n",
       "        [[-0.3733, -0.8585]],\n",
       "\n",
       "        [[-0.4237, -0.7637]],\n",
       "\n",
       "        [[-0.6484, -0.2778]],\n",
       "\n",
       "        [[-0.5567, -0.8247]],\n",
       "\n",
       "        [[-0.6161, -1.2343]],\n",
       "\n",
       "        [[-0.6695, -0.3852]],\n",
       "\n",
       "        [[-0.6328, -0.7922]],\n",
       "\n",
       "        [[-0.6597, -1.1328]],\n",
       "\n",
       "        [[-0.6509, -1.3098]],\n",
       "\n",
       "        [[-0.6534, -0.3330]],\n",
       "\n",
       "        [[-0.5704, -1.1740]],\n",
       "\n",
       "        [[-0.6532, -0.2968]],\n",
       "\n",
       "        [[-0.5602, -0.3840]],\n",
       "\n",
       "        [[-0.6507, -0.8223]],\n",
       "\n",
       "        [[-0.6980, -1.1214]],\n",
       "\n",
       "        [[-0.8199, -0.2763]],\n",
       "\n",
       "        [[-0.7413, -0.5248]],\n",
       "\n",
       "        [[-0.6266, -0.6431]],\n",
       "\n",
       "        [[-0.6043, -0.4830]],\n",
       "\n",
       "        [[-0.6659, -0.3691]],\n",
       "\n",
       "        [[-0.7250, -0.6847]],\n",
       "\n",
       "        [[-0.8687, -0.2815]],\n",
       "\n",
       "        [[-0.7450, -1.1307]],\n",
       "\n",
       "        [[-0.7958, -0.9478]],\n",
       "\n",
       "        [[-0.9325, -0.3313]],\n",
       "\n",
       "        [[-0.7449, -1.0174]],\n",
       "\n",
       "        [[-0.8355, -0.6811]],\n",
       "\n",
       "        [[-0.8612, -0.5313]],\n",
       "\n",
       "        [[-0.7897, -0.5800]],\n",
       "\n",
       "        [[-0.6556, -1.1758]],\n",
       "\n",
       "        [[-0.6273, -0.6653]],\n",
       "\n",
       "        [[-0.6613, -0.3335]],\n",
       "\n",
       "        [[-0.6743, -0.4168]],\n",
       "\n",
       "        [[-0.6701, -0.6078]],\n",
       "\n",
       "        [[-0.6526, -1.0416]],\n",
       "\n",
       "        [[-0.6125, -0.7622]],\n",
       "\n",
       "        [[-0.6807, -0.2897]],\n",
       "\n",
       "        [[-0.6683, -0.7809]],\n",
       "\n",
       "        [[-1.1879, -1.3316]],\n",
       "\n",
       "        [[-1.3935, -1.2253]],\n",
       "\n",
       "        [[-1.5979, -0.2289]],\n",
       "\n",
       "        [[-1.1530, -1.1260]],\n",
       "\n",
       "        [[-0.8902, -1.0397]],\n",
       "\n",
       "        [[-0.7165, -0.8901]],\n",
       "\n",
       "        [[-0.6451, -0.7481]],\n",
       "\n",
       "        [[-0.6243, -0.6553]],\n",
       "\n",
       "        [[-0.5754, -0.6324]],\n",
       "\n",
       "        [[-0.4892, -0.6503]],\n",
       "\n",
       "        [[-0.4013, -0.6756]],\n",
       "\n",
       "        [[-0.3498, -0.6485]],\n",
       "\n",
       "        [[-0.3564, -0.5825]],\n",
       "\n",
       "        [[-0.4175, -0.5061]],\n",
       "\n",
       "        [[-0.4969, -0.4683]],\n",
       "\n",
       "        [[-0.5663, -0.4769]],\n",
       "\n",
       "        [[-0.6137, -0.5047]],\n",
       "\n",
       "        [[-0.6382, -0.5145]],\n",
       "\n",
       "        [[-0.6663, -0.5062]],\n",
       "\n",
       "        [[-0.6968, -0.5143]],\n",
       "\n",
       "        [[-0.6552, -0.5652]],\n",
       "\n",
       "        [[-0.5761, -0.6174]],\n",
       "\n",
       "        [[-0.5138, -0.6203]],\n",
       "\n",
       "        [[-0.5003, -0.5458]],\n",
       "\n",
       "        [[-0.5453, -0.4543]],\n",
       "\n",
       "        [[-0.6569, -0.3766]],\n",
       "\n",
       "        [[-0.7840, -0.3580]],\n",
       "\n",
       "        [[-0.8182, -0.4191]],\n",
       "\n",
       "        [[-0.7387, -0.5387]],\n",
       "\n",
       "        [[-0.6274, -0.6365]],\n",
       "\n",
       "        [[-0.5714, -0.6586]],\n",
       "\n",
       "        [[-0.6111, -0.6075]],\n",
       "\n",
       "        [[-0.7467, -0.5205]],\n",
       "\n",
       "        [[-0.8880, -0.4491]],\n",
       "\n",
       "        [[-0.9668, -0.4204]],\n",
       "\n",
       "        [[-0.9740, -0.3964]],\n",
       "\n",
       "        [[-0.9195, -0.3687]],\n",
       "\n",
       "        [[-0.8888, -0.3389]],\n",
       "\n",
       "        [[-0.8956, -0.3335]],\n",
       "\n",
       "        [[-0.8854, -0.3616]],\n",
       "\n",
       "        [[-0.8111, -0.4186]],\n",
       "\n",
       "        [[-0.6998, -0.4776]],\n",
       "\n",
       "        [[-0.6257, -0.5071]],\n",
       "\n",
       "        [[-0.6132, -0.5057]],\n",
       "\n",
       "        [[-0.6523, -0.5046]],\n",
       "\n",
       "        [[-0.6838, -0.5224]],\n",
       "\n",
       "        [[-0.6608, -0.5411]],\n",
       "\n",
       "        [[-0.6079, -0.5297]],\n",
       "\n",
       "        [[-0.5940, -0.4696]],\n",
       "\n",
       "        [[-0.6600, -0.3903]]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {k: v.detach() for k, v in tqs.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in tqs.named_buffers()}\n",
    "\n",
    "lnP = tqs._ln_P(a_spin_chain, a_potential_func, params, buffers)\n",
    "lnP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the derivative of tqs._lnP with respect to a single spin chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_flash_attention_for_cpu_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1718580740865/work/aten/src/ATen/functorch/BatchedFallback.cpp:81.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-6.9693e-04, -8.1649e-04, -1.7328e-03,  ..., -3.1597e-04,\n",
       "           -3.0234e-04, -2.9149e-04],\n",
       "          [-6.0595e-04, -6.6552e-04, -1.5533e-03,  ..., -3.9051e-04,\n",
       "           -3.5299e-04, -3.3270e-04]]],\n",
       "\n",
       "\n",
       "        [[[-7.7657e-04, -8.8852e-04, -2.0411e-03,  ..., -5.5390e-04,\n",
       "           -6.3683e-04, -6.1466e-04],\n",
       "          [-4.5378e-03, -4.5588e-03, -9.6902e-03,  ..., -4.7605e-03,\n",
       "           -4.8577e-03, -4.6207e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.0534e-03, -1.1253e-03, -2.6640e-03,  ..., -5.3529e-04,\n",
       "           -5.7606e-04, -5.7928e-04],\n",
       "          [-8.6301e-04, -8.7886e-04, -2.3665e-03,  ..., -7.0268e-04,\n",
       "           -6.9369e-04, -6.6843e-04]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-9.9364e-04, -1.0009e-03, -2.8760e-03,  ..., -8.5855e-01,\n",
       "           -5.3808e-04, -5.8313e-04],\n",
       "          [-1.9138e-03, -1.8252e-03, -5.5144e-03,  ...,  1.4042e+00,\n",
       "           -1.7142e-03, -1.8204e-03]]],\n",
       "\n",
       "\n",
       "        [[[-6.6645e-04, -6.4199e-04, -1.5567e-03,  ..., -1.8454e-04,\n",
       "           -7.6325e-01, -3.5101e-04],\n",
       "          [-1.6663e-03, -1.6123e-03, -4.1268e-03,  ..., -1.3958e-03,\n",
       "            1.3355e+00, -1.7121e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.2295e-04, -4.9580e-04, -1.1465e-03,  ..., -6.5354e-05,\n",
       "           -1.7189e-04, -8.6676e-01],\n",
       "          [-1.2498e-03, -1.2004e-03, -2.9696e-03,  ..., -1.0484e-03,\n",
       "           -1.1816e-03,  1.1813e+00]]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {k: v.detach() for k, v in tqs.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in tqs.named_buffers()}\n",
    "deriv = jacrev(tqs._ln_P, argnums=0)(a_spin_chain, a_potential_func, params, buffers)\n",
    "deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...so now we need to map this single-spin-chain `jacrev(tqs)` operator over the spin chains in our batch\n",
    "- (noting that `jacrev(tqs)` is *itself* an operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlnP_dTheta_sample = jacrev(tqs._ln_P)\n",
    "vmap_dlnP_dTheta = vmap(dlnP_dTheta_sample, in_dims=(1, 1, None, None))\n",
    "vmap_dlnP_dTheta(a_spin_chain, a_potential_func, params, buffers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(d_ln_P_d_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([192])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([192])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters shapes of the model\n",
    "for i in tqs.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.9367,  0.2668, -1.6303, -1.7862,  0.8277,  0.1077,  1.8067, -2.5265,\n",
      "         0.7316, -1.4697, -0.4042, -0.8399, -0.2293,  0.3884,  0.7997,  1.0304,\n",
      "         0.6088, -0.0918, -0.6525, -0.7985,  0.5418,  0.5808,  0.5402,  0.6873,\n",
      "         0.1134,  0.5988, -1.9142, -0.8318,  0.6114, -0.2724, -0.2606, -1.3139,\n",
      "         1.0847,  1.9056, -1.2572,  1.5666,  0.2288, -0.4180,  0.6461, -0.3176,\n",
      "        -0.3120, -0.2958, -0.3488, -0.3325, -0.6533, -0.8155, -0.2047,  0.2941,\n",
      "        -1.8817, -2.4597, -1.3084,  1.9289,  1.8321, -0.0902,  0.4203,  1.6066,\n",
      "        -0.7052, -0.0258, -0.1230,  0.3112,  1.2757,  0.0777, -1.9691, -0.9766],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4751, -0.3406,  0.2423,  0.2803,  1.7058,  0.7552, -0.9422,  0.0115,\n",
      "         0.9143,  0.5812,  0.8512, -0.0914,  0.6948, -0.4445,  0.3403, -1.9351,\n",
      "        -0.4757, -0.5741, -1.0688,  0.9787, -0.3464, -0.4479,  1.8879, -0.2108,\n",
      "         1.2273,  0.2867,  0.2962, -1.8591,  1.5110,  0.1349, -0.5054,  1.1011,\n",
      "        -1.2408, -0.1704,  0.2568,  2.4227, -0.5757,  0.9428,  0.1160, -0.5598,\n",
      "         1.3906, -1.3730, -1.4973,  2.1498,  1.0838,  1.2060,  0.0827, -0.4469,\n",
      "        -1.2853, -0.4454, -1.0298, -0.3567,  1.2075,  0.2536,  0.3128,  1.4835,\n",
      "         0.6587, -0.1873,  0.0933,  0.9059, -0.5005,  1.1766, -1.0223, -0.6396],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1149, -0.1347, -0.0496,  ..., -0.1256,  0.1527,  0.1075],\n",
      "        [ 0.0944, -0.0440, -0.0170,  ..., -0.1480, -0.1399,  0.0873],\n",
      "        [ 0.1318, -0.0547, -0.1052,  ..., -0.0511, -0.0090, -0.0238],\n",
      "        ...,\n",
      "        [ 0.1293,  0.0152, -0.0326,  ..., -0.1025, -0.0556, -0.0271],\n",
      "        [ 0.1436, -0.1439, -0.1245,  ...,  0.0227, -0.0246,  0.0381],\n",
      "        [ 0.0909,  0.0142,  0.1043,  ..., -0.0937, -0.0422, -0.0643]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1062,  0.0109, -0.1124,  ..., -0.0729, -0.0937,  0.0368],\n",
      "        [ 0.1176,  0.0314, -0.1170,  ...,  0.0160, -0.0010, -0.0082],\n",
      "        [ 0.0211, -0.0214,  0.0657,  ...,  0.1021,  0.0578, -0.0588],\n",
      "        ...,\n",
      "        [-0.0508,  0.0651,  0.0006,  ...,  0.0862,  0.0667, -0.0229],\n",
      "        [-0.0698, -0.0941,  0.0464,  ..., -0.0796, -0.1065,  0.0467],\n",
      "        [ 0.0699, -0.0922,  0.1235,  ...,  0.0882,  0.0830, -0.0017]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0306, -0.0249, -0.1003,  ..., -0.0059,  0.0440,  0.0800],\n",
      "        [ 0.0211, -0.1176,  0.0246,  ..., -0.1231, -0.0766,  0.0786],\n",
      "        [-0.0206, -0.0829,  0.1160,  ...,  0.0108,  0.1110, -0.0402],\n",
      "        ...,\n",
      "        [ 0.1221,  0.0444,  0.0392,  ..., -0.0126,  0.0745, -0.1012],\n",
      "        [ 0.0357,  0.1098, -0.0641,  ...,  0.1066,  0.1060, -0.0197],\n",
      "        [-0.0472,  0.0338,  0.0647,  ...,  0.0738, -0.0995,  0.0832]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0008,  0.0911, -0.0113,  0.1153,  0.1171,  0.0311, -0.0924,  0.0237,\n",
      "         0.0023, -0.0386,  0.0856, -0.0203, -0.0233, -0.1138, -0.0206, -0.0763,\n",
      "         0.0215, -0.0423,  0.0174,  0.0516, -0.0791, -0.0496,  0.0126,  0.0663,\n",
      "        -0.0018, -0.0627,  0.0235, -0.0009,  0.0128, -0.0129,  0.0020,  0.0974,\n",
      "         0.1179,  0.0754, -0.0162, -0.0137, -0.0701, -0.0924,  0.0999,  0.0522,\n",
      "        -0.0044, -0.0404,  0.0304,  0.0730,  0.1178,  0.0276,  0.0704, -0.0837,\n",
      "        -0.0858, -0.1130, -0.0430, -0.0718, -0.0678,  0.0016, -0.0384, -0.0156,\n",
      "         0.0390, -0.0676,  0.1237,  0.1005,  0.0481,  0.0152, -0.0381,  0.0277,\n",
      "         0.0509,  0.0216, -0.1022,  0.0772, -0.0361, -0.1023, -0.0942, -0.0157,\n",
      "        -0.0652,  0.0941, -0.0559, -0.0268,  0.0633, -0.0403,  0.0709,  0.0996,\n",
      "         0.0144, -0.0366,  0.0572, -0.0521, -0.0048, -0.0786,  0.0161, -0.0230,\n",
      "         0.0797, -0.0851, -0.0757,  0.1200,  0.0874,  0.1189,  0.0557, -0.0290,\n",
      "        -0.0927,  0.0307, -0.0427,  0.0793, -0.0448, -0.0563,  0.0929, -0.1033,\n",
      "         0.0317, -0.0581, -0.0173,  0.0718,  0.0926,  0.0088,  0.1004,  0.0304,\n",
      "        -0.0071,  0.0549,  0.0532,  0.0830, -0.0180,  0.0772, -0.0814, -0.0578,\n",
      "         0.0324,  0.1022, -0.1222, -0.1185, -0.1069,  0.0104,  0.0588, -0.0220],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0207,  0.0600,  0.0130,  ..., -0.0748, -0.0222, -0.0631],\n",
      "        [ 0.0419,  0.0116,  0.0649,  ..., -0.0311,  0.0588,  0.0235],\n",
      "        [-0.0103, -0.0396,  0.0598,  ...,  0.0085,  0.0425,  0.0467],\n",
      "        ...,\n",
      "        [-0.0869, -0.0407, -0.0674,  ...,  0.0753,  0.0837,  0.0140],\n",
      "        [ 0.0252,  0.0235, -0.0638,  ..., -0.0686,  0.0269, -0.0069],\n",
      "        [-0.0847,  0.0605,  0.0573,  ...,  0.0326, -0.0629, -0.0440]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0776,  0.0346,  0.0793,  0.0731,  0.0696,  0.0030, -0.0377, -0.0556,\n",
      "         0.0344, -0.0548,  0.0184, -0.0321,  0.0742, -0.0556,  0.0409,  0.0419,\n",
      "        -0.0238,  0.0198,  0.0033,  0.0793,  0.0832, -0.0613, -0.0249,  0.0132,\n",
      "         0.0736, -0.0222,  0.0871, -0.0441,  0.0734, -0.0464,  0.0500,  0.0376,\n",
      "        -0.0742,  0.0844,  0.0290,  0.0692, -0.0649, -0.0116,  0.0810, -0.0610,\n",
      "        -0.0787,  0.0497, -0.0273,  0.0241, -0.0079, -0.0389,  0.0428,  0.0281,\n",
      "        -0.0169,  0.0247, -0.0392,  0.0708,  0.0741,  0.0104,  0.0397, -0.0313,\n",
      "         0.0338,  0.0514, -0.0297,  0.0704,  0.0108, -0.0238, -0.0828,  0.0850],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1149, -0.1347, -0.0496,  ..., -0.1256,  0.1527,  0.1075],\n",
      "        [ 0.0944, -0.0440, -0.0170,  ..., -0.1480, -0.1399,  0.0873],\n",
      "        [ 0.1318, -0.0547, -0.1052,  ..., -0.0511, -0.0090, -0.0238],\n",
      "        ...,\n",
      "        [ 0.1293,  0.0152, -0.0326,  ..., -0.1025, -0.0556, -0.0271],\n",
      "        [ 0.1436, -0.1439, -0.1245,  ...,  0.0227, -0.0246,  0.0381],\n",
      "        [ 0.0909,  0.0142,  0.1043,  ..., -0.0937, -0.0422, -0.0643]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1062,  0.0109, -0.1124,  ..., -0.0729, -0.0937,  0.0368],\n",
      "        [ 0.1176,  0.0314, -0.1170,  ...,  0.0160, -0.0010, -0.0082],\n",
      "        [ 0.0211, -0.0214,  0.0657,  ...,  0.1021,  0.0578, -0.0588],\n",
      "        ...,\n",
      "        [-0.0508,  0.0651,  0.0006,  ...,  0.0862,  0.0667, -0.0229],\n",
      "        [-0.0698, -0.0941,  0.0464,  ..., -0.0796, -0.1065,  0.0467],\n",
      "        [ 0.0699, -0.0922,  0.1235,  ...,  0.0882,  0.0830, -0.0017]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0306, -0.0249, -0.1003,  ..., -0.0059,  0.0440,  0.0800],\n",
      "        [ 0.0211, -0.1176,  0.0246,  ..., -0.1231, -0.0766,  0.0786],\n",
      "        [-0.0206, -0.0829,  0.1160,  ...,  0.0108,  0.1110, -0.0402],\n",
      "        ...,\n",
      "        [ 0.1221,  0.0444,  0.0392,  ..., -0.0126,  0.0745, -0.1012],\n",
      "        [ 0.0357,  0.1098, -0.0641,  ...,  0.1066,  0.1060, -0.0197],\n",
      "        [-0.0472,  0.0338,  0.0647,  ...,  0.0738, -0.0995,  0.0832]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0008,  0.0911, -0.0113,  0.1153,  0.1171,  0.0311, -0.0924,  0.0237,\n",
      "         0.0023, -0.0386,  0.0856, -0.0203, -0.0233, -0.1138, -0.0206, -0.0763,\n",
      "         0.0215, -0.0423,  0.0174,  0.0516, -0.0791, -0.0496,  0.0126,  0.0663,\n",
      "        -0.0018, -0.0627,  0.0235, -0.0009,  0.0128, -0.0129,  0.0020,  0.0974,\n",
      "         0.1179,  0.0754, -0.0162, -0.0137, -0.0701, -0.0924,  0.0999,  0.0522,\n",
      "        -0.0044, -0.0404,  0.0304,  0.0730,  0.1178,  0.0276,  0.0704, -0.0837,\n",
      "        -0.0858, -0.1130, -0.0430, -0.0718, -0.0678,  0.0016, -0.0384, -0.0156,\n",
      "         0.0390, -0.0676,  0.1237,  0.1005,  0.0481,  0.0152, -0.0381,  0.0277,\n",
      "         0.0509,  0.0216, -0.1022,  0.0772, -0.0361, -0.1023, -0.0942, -0.0157,\n",
      "        -0.0652,  0.0941, -0.0559, -0.0268,  0.0633, -0.0403,  0.0709,  0.0996,\n",
      "         0.0144, -0.0366,  0.0572, -0.0521, -0.0048, -0.0786,  0.0161, -0.0230,\n",
      "         0.0797, -0.0851, -0.0757,  0.1200,  0.0874,  0.1189,  0.0557, -0.0290,\n",
      "        -0.0927,  0.0307, -0.0427,  0.0793, -0.0448, -0.0563,  0.0929, -0.1033,\n",
      "         0.0317, -0.0581, -0.0173,  0.0718,  0.0926,  0.0088,  0.1004,  0.0304,\n",
      "        -0.0071,  0.0549,  0.0532,  0.0830, -0.0180,  0.0772, -0.0814, -0.0578,\n",
      "         0.0324,  0.1022, -0.1222, -0.1185, -0.1069,  0.0104,  0.0588, -0.0220],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0207,  0.0600,  0.0130,  ..., -0.0748, -0.0222, -0.0631],\n",
      "        [ 0.0419,  0.0116,  0.0649,  ..., -0.0311,  0.0588,  0.0235],\n",
      "        [-0.0103, -0.0396,  0.0598,  ...,  0.0085,  0.0425,  0.0467],\n",
      "        ...,\n",
      "        [-0.0869, -0.0407, -0.0674,  ...,  0.0753,  0.0837,  0.0140],\n",
      "        [ 0.0252,  0.0235, -0.0638,  ..., -0.0686,  0.0269, -0.0069],\n",
      "        [-0.0847,  0.0605,  0.0573,  ...,  0.0326, -0.0629, -0.0440]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0776,  0.0346,  0.0793,  0.0731,  0.0696,  0.0030, -0.0377, -0.0556,\n",
      "         0.0344, -0.0548,  0.0184, -0.0321,  0.0742, -0.0556,  0.0409,  0.0419,\n",
      "        -0.0238,  0.0198,  0.0033,  0.0793,  0.0832, -0.0613, -0.0249,  0.0132,\n",
      "         0.0736, -0.0222,  0.0871, -0.0441,  0.0734, -0.0464,  0.0500,  0.0376,\n",
      "        -0.0742,  0.0844,  0.0290,  0.0692, -0.0649, -0.0116,  0.0810, -0.0610,\n",
      "        -0.0787,  0.0497, -0.0273,  0.0241, -0.0079, -0.0389,  0.0428,  0.0281,\n",
      "        -0.0169,  0.0247, -0.0392,  0.0708,  0.0741,  0.0104,  0.0397, -0.0313,\n",
      "         0.0338,  0.0514, -0.0297,  0.0704,  0.0108, -0.0238, -0.0828,  0.0850],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0340, -0.0425, -0.0054,  0.1040,  0.1203,  0.0581,  0.0395, -0.0806,\n",
      "         -0.0247,  0.0101,  0.0968,  0.0908, -0.0726,  0.0258,  0.0658, -0.0174,\n",
      "          0.0691,  0.0265, -0.0410, -0.0930,  0.0755,  0.0145,  0.0006, -0.0982,\n",
      "          0.0692, -0.0786,  0.0928, -0.0601,  0.0976, -0.0815, -0.0071,  0.0812,\n",
      "         -0.0508, -0.0516, -0.1035, -0.0236, -0.0313,  0.0918,  0.0088,  0.0004,\n",
      "         -0.0241,  0.0384, -0.0074,  0.1222, -0.0451,  0.0739, -0.0247,  0.0151,\n",
      "          0.0092, -0.0535, -0.1100, -0.0001, -0.0967,  0.0237, -0.0129,  0.1105,\n",
      "          0.0948,  0.0362, -0.1009,  0.0856, -0.1028,  0.1130,  0.0572,  0.0427],\n",
      "        [-0.0677,  0.0234,  0.0820, -0.1117,  0.0244, -0.0408,  0.0728,  0.0321,\n",
      "         -0.1027, -0.0101,  0.0037,  0.0585, -0.0635,  0.0341, -0.0280, -0.0402,\n",
      "         -0.0872,  0.0893,  0.0306, -0.0247,  0.1050,  0.1040,  0.0165, -0.1001,\n",
      "         -0.1029, -0.0873,  0.1131, -0.0796,  0.0870,  0.0185,  0.1190,  0.0269,\n",
      "         -0.0368,  0.0401, -0.0441,  0.0808, -0.0197,  0.0168, -0.0033,  0.1197,\n",
      "          0.1091, -0.0640, -0.0926,  0.0246, -0.0253,  0.0938,  0.0569,  0.0197,\n",
      "         -0.1231, -0.0273,  0.0928, -0.0922, -0.0781, -0.0786, -0.0679,  0.0266,\n",
      "          0.0848,  0.0243, -0.1164, -0.0722, -0.0102,  0.0094, -0.0417,  0.0714]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0342, -0.0953], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0223, -0.0473, -0.0334, -0.0805,  0.0329, -0.1023, -0.1101, -0.1047,\n",
      "         -0.0447, -0.0402,  0.0592, -0.0387,  0.0175,  0.0406, -0.0143, -0.1007,\n",
      "         -0.0161,  0.0081, -0.0155,  0.0272, -0.0520, -0.0450,  0.0124,  0.0935,\n",
      "          0.0562, -0.0714,  0.1098, -0.0240, -0.0999, -0.0235, -0.0668, -0.0268,\n",
      "         -0.0995,  0.0922, -0.0052, -0.0761,  0.0055,  0.1096,  0.1188,  0.0370,\n",
      "          0.0851, -0.0887, -0.0237, -0.0856,  0.0426, -0.0323,  0.0952,  0.0578,\n",
      "          0.1157,  0.0449,  0.0859, -0.1074,  0.0941, -0.0519, -0.1156,  0.0317,\n",
      "          0.0389,  0.1086,  0.1094,  0.1241,  0.0286,  0.0927, -0.0611,  0.0182]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1078], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in tqs.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([192, 64]) torch.Size([192, 64])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([64, 64]) torch.Size([64, 64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([128, 64]) torch.Size([128, 64])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([64, 128]) torch.Size([64, 128])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([2, 64]) torch.Size([2, 64])\n",
      "torch.Size([2]) torch.Size([2])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for grad, param in zip(d_ln_P_d_theta, tqs.parameters()):\n",
    "    if grad is not None:\n",
    "        print(grad.shape, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 12.0460,  -5.3368,  -5.4714,   6.7329, -21.8639,  16.2115,  -1.8841,\n",
       "           2.7430,   6.1641,  25.9044,  -1.0642,   8.0647, -14.6524,   1.7242,\n",
       "           3.3404,  19.5245,  -1.9994,  16.3765,   3.0707,  -3.7363,   5.9793,\n",
       "          -3.7739,  16.6138,  -8.9862,   9.6171, -33.6170,  14.7179,  -7.1137,\n",
       "          10.2698,   0.3457,   9.2687,   1.9637,  -3.1346, -19.9616,   6.5355,\n",
       "           5.2249,  11.2312,   8.6126,  -4.7546,   2.2451,   3.1444, -15.6841,\n",
       "          -0.5689,  -1.4260,  23.9463,   7.9263,   2.7100,   7.9186,  -0.6613,\n",
       "           6.2767,  -8.4629,  -2.8857,  -9.8538,   7.5645, -11.8798,  13.5053,\n",
       "          -9.0124,  16.5284,  -0.6507,  14.4156, -19.2918,  -6.8954,  -2.0077,\n",
       "           8.1826], grad_fn=<ViewBackward0>),\n",
       " tensor([-1.4482,  0.8351,  1.4949, -1.6435,  0.7335, -1.1153,  0.9714,  0.2423,\n",
       "         -2.1453, -1.2152,  0.3682,  0.1144, -0.5094,  0.1646, -0.4671, -0.9084,\n",
       "         -1.0607,  0.3759,  0.3585,  0.0600,  0.9356,  1.6283, -0.6500, -0.7605,\n",
       "         -1.3930,  0.1979,  0.8508, -0.2695, -0.0847,  0.0198,  1.3698, -0.1712,\n",
       "         -0.4297,  1.7634, -0.4233,  0.4039, -0.6001,  0.1792,  0.2397,  1.2434,\n",
       "          1.2910, -0.7106, -0.9060, -0.2231, -1.1202,  0.5364,  0.1718,  0.5383,\n",
       "         -1.6035, -0.4522,  1.5999, -1.3570, -0.3505, -1.1142, -0.7784, -0.6054,\n",
       "          1.0357, -0.2342, -1.3254, -1.8747,  0.2195, -0.0214, -0.8010,  0.6199],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " tensor([[-1.0758e+00, -3.2617e-01,  9.3843e-01,  ..., -2.9942e+01,\n",
       "           1.5681e-01, -2.9484e+01],\n",
       "         [ 1.1306e-01,  7.4768e-03, -7.0891e-02,  ...,  3.2736e+00,\n",
       "          -2.0165e-02,  3.2188e+00],\n",
       "         [-5.7658e-02,  1.1248e-01,  1.3937e-02,  ..., -1.7406e+00,\n",
       "           2.0713e-02, -1.6952e+00],\n",
       "         ...,\n",
       "         [-1.4849e+01, -6.0784e+00,  3.1440e+01,  ...,  9.0570e+01,\n",
       "           3.6458e+01,  1.0877e+02],\n",
       "         [-8.8040e+00, -3.6471e+00,  1.8761e+01,  ...,  5.7345e+01,\n",
       "           2.1715e+01,  6.8139e+01],\n",
       "         [ 7.2676e+00,  3.0779e+00, -1.5688e+01,  ..., -4.9337e+01,\n",
       "          -1.8128e+01, -5.8318e+01]], grad_fn=<TBackward0>),\n",
       " tensor([-2.9646e+01,  3.2381e+00, -1.7112e+00,  1.5958e+01,  1.9689e+01,\n",
       "         -4.3722e+01, -3.1969e+01,  2.3443e+01,  2.5034e+01, -1.5603e+01,\n",
       "         -1.4305e+01,  2.5567e+01, -2.5172e+00,  1.6268e+01,  5.1950e+00,\n",
       "         -3.6194e+01, -2.4798e+00, -2.6066e+01, -6.8854e+00,  3.1431e+01,\n",
       "          3.7500e+01, -9.5716e+00,  6.5639e+00,  9.9447e+00,  1.7616e+00,\n",
       "          2.7414e+01,  3.7529e+01,  1.3813e+01,  5.5600e+00,  1.4701e+01,\n",
       "         -4.0678e+01,  1.8636e+01,  9.7607e+00, -2.3572e+01, -8.9726e+00,\n",
       "         -2.5326e+01, -2.2017e+01,  1.8576e+01,  3.1490e+00, -3.6929e+01,\n",
       "          1.2913e+01, -1.0100e+01,  1.3343e+01,  7.8212e-01, -5.6526e+01,\n",
       "         -1.6500e+01, -6.2152e+00, -2.0195e+01, -1.7589e+01, -3.1791e+00,\n",
       "         -4.0567e+01,  5.2217e+01, -2.4828e+01,  1.4646e+01,  5.8255e+01,\n",
       "         -1.4706e+01,  9.4856e+00, -2.4924e+01, -2.9111e+01, -9.5584e+00,\n",
       "         -3.9023e+01,  3.7101e+01, -1.8337e+00, -2.5471e+01,  6.8918e-08,\n",
       "         -6.7055e-08,  1.0058e-07,  1.6391e-07, -8.3819e-08,  3.3528e-08,\n",
       "         -2.9802e-08, -1.0431e-07,  0.0000e+00,  1.6764e-08,  2.0862e-07,\n",
       "          8.1956e-08, -2.7940e-09,  8.1956e-08,  6.3330e-08, -2.7940e-08,\n",
       "         -7.8697e-08, -7.4506e-08, -3.3528e-08,  7.1712e-08,  3.2037e-07,\n",
       "         -3.7253e-08, -2.0117e-07,  3.6508e-07,  1.4901e-07, -1.5646e-07,\n",
       "          2.6077e-08,  5.8115e-07,  2.0489e-08,  1.7136e-07,  3.7253e-08,\n",
       "         -4.8429e-08, -4.2841e-08, -3.7253e-08,  2.9802e-08, -1.0803e-07,\n",
       "         -4.4703e-08,  3.7253e-08, -6.7055e-08,  0.0000e+00, -7.4506e-08,\n",
       "          5.2154e-08,  2.0489e-08,  3.3528e-08, -8.5682e-08,  5.9605e-07,\n",
       "          1.6391e-07,  0.0000e+00,  1.1921e-07, -1.7695e-08, -5.9605e-08,\n",
       "          2.6077e-07,  3.9116e-08,  8.1956e-08, -5.2154e-08, -2.0862e-07,\n",
       "          1.5646e-07,  4.2841e-08, -4.0978e-08, -2.9802e-08,  1.4901e-07,\n",
       "          1.9744e-07,  2.9802e-08,  1.3411e-07,  1.5741e+01,  8.4038e+01,\n",
       "         -7.8192e+01,  7.1320e+01, -1.0958e+02,  1.0698e+02, -6.0031e+01,\n",
       "         -3.0038e+00,  3.7204e+00, -6.2448e+01, -2.3399e+01,  3.9813e+01,\n",
       "          4.2283e+01, -4.7650e+01,  3.3535e+01,  5.8268e+00,  4.6890e+01,\n",
       "         -1.7110e+02,  1.7624e+00,  6.2257e+01, -6.8823e+00, -4.7385e+01,\n",
       "          6.4677e+01, -5.0766e+01, -2.8569e+01, -9.5344e+01, -3.5944e+01,\n",
       "         -3.0294e+00,  1.2633e+01,  8.3078e+00,  1.4016e+01,  8.6170e+01,\n",
       "         -1.7685e+01, -3.8470e+01,  2.5520e+01, -2.6657e+01, -1.2764e+01,\n",
       "         -4.8096e+01,  7.6159e-01, -6.7124e+01, -2.2995e+01, -2.2212e+01,\n",
       "          3.7169e+01, -1.3690e+01,  7.6815e+00,  1.9460e+01,  6.7804e+00,\n",
       "         -7.3712e+01, -6.1495e+00,  2.9846e+01,  1.6483e+01, -9.7684e+00,\n",
       "         -8.4677e+01, -8.1769e+01,  5.8071e+01, -5.1772e+01,  4.0807e+01,\n",
       "         -2.8450e+01,  1.3667e+01, -2.7248e+01, -6.1022e+01,  9.0971e+01,\n",
       "          5.7545e+01, -4.9480e+01], grad_fn=<ViewBackward0>),\n",
       " tensor([[  0.7543,  41.4062, -33.3401,  ...,   2.9278,  14.7004, -35.8029],\n",
       "         [  3.2872, -17.4195,  20.1845,  ...,   1.0794,  -5.4357,  15.9621],\n",
       "         [  0.2644, -57.4890,  48.5597,  ...,  -3.2457, -20.1536,  50.0487],\n",
       "         ...,\n",
       "         [ -0.4775,  -1.6958,   0.6098,  ...,  -0.3881,  -0.6594,   1.3547],\n",
       "         [ -1.0715,  36.7660, -32.5803,  ...,   1.4847,  12.7257, -32.2213],\n",
       "         [ -0.3656, -60.5149,  49.9895,  ...,  -3.8061, -21.3686,  52.5022]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([ -71.5833,   36.5131,  101.7186, -137.0842,   20.2367,  -30.1991,\n",
       "           80.7271,   41.2087, -147.1909,   -5.8409,   29.8020,   65.2213,\n",
       "          -50.8602,   22.8549,  -17.7327,  -46.2873,  -79.5682,   92.7451,\n",
       "           50.2928,  -21.2522,  124.6082,  125.4182,   -3.4944, -125.0116,\n",
       "         -134.1794, -128.3991,  133.0302,  -73.9804,   98.8873,   15.6790,\n",
       "          154.4969,   27.5897,  -64.9665,   58.8066,  -50.2947,  105.6436,\n",
       "          -31.3541,   43.4165,   -4.0200,  182.4493,  115.6465,  -85.5516,\n",
       "         -121.1594,   13.1944,  -50.4834,  105.4286,   93.8919,   28.9109,\n",
       "         -167.7226,  -26.7217,  139.7401, -125.3938,  -77.0902, -108.9787,\n",
       "          -89.6940,   28.9960,  129.1182,   49.9690, -148.5907, -103.3577,\n",
       "          -29.6247,    2.1512,  -66.6310,  105.9062], grad_fn=<ViewBackward0>),\n",
       " tensor([[ 14.1009,  23.5426,  12.7609,  ..., -16.3514,  11.8230, -24.8607],\n",
       "         [ 24.5244,  42.8022,  17.5638,  ..., -27.2272,  20.0145, -39.9431],\n",
       "         [ 16.7714,  28.6231,  12.7476,  ..., -20.4391,  15.3355, -29.4084],\n",
       "         ...,\n",
       "         [  0.1455,   1.3759,   0.3461,  ...,  -0.9039,   0.8036,  -1.1609],\n",
       "         [  2.5182,   4.6887,   3.3722,  ...,  -3.7850,   2.8445,  -5.1906],\n",
       "         [-11.2469, -14.1559,  -6.5855,  ...,  10.8203,  -7.8174,  17.3501]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([-20.5468, -34.1049, -25.7576,  -3.0904,  -1.2963,   2.6892,  -0.0928,\n",
       "          -8.1693,  23.0133, -31.8499,   0.0000,  -8.5333,   6.9477,   0.0000,\n",
       "          62.1644,   2.4966,  -2.6567,  -0.7705,   0.5204,  10.3993,  -1.5965,\n",
       "           0.6094,   0.0000,   0.5364,  -5.1855,   7.5987,  13.3557, -10.1663,\n",
       "           0.3133,  15.0105,  -3.6924,   4.7107,  22.5416, -11.8616, -11.5486,\n",
       "           0.1603,   2.8340,   2.5026,  -0.9501,   0.7625, -24.4779,  -0.8888,\n",
       "          18.8728, -44.2703,   2.4760,  -1.6480,  -1.1347, -13.9856, -12.9660,\n",
       "         -23.8267,  -2.7003,   3.7520,  -9.9994,   6.4425,   0.0656,   0.0000,\n",
       "          -8.0126,   0.3047, -14.8097, -38.8014,  -2.0307,  41.0631,   0.6843,\n",
       "         -16.2779,   2.0267,   1.9488,   1.9704,  -1.4131,   0.8928,  -6.4718,\n",
       "           5.0564,  -0.4639, -16.3344,   0.7584,  -3.6426,   2.9900,  17.9703,\n",
       "          -6.0191,   2.2837,   1.6199,  -2.2729,  32.6460,  -2.2367,  49.0439,\n",
       "          -0.1463,   0.0000,  -7.6724, -12.6036,   4.1669,   3.4699,   8.4972,\n",
       "          13.4856,   1.7222, -10.2489, -17.4309,  -2.1514,   0.0000, -35.3052,\n",
       "         -11.4870,   8.0258,  -8.0351,   1.7834,  -8.0925, -10.7298,   1.4476,\n",
       "          11.6297,  -1.3037,  -1.3422,  -0.9387, -35.1420,  10.4566,   0.2983,\n",
       "          -1.3407, -20.6899,  -4.3405,  12.8082,  25.7544,   7.8190,   1.4159,\n",
       "           0.8801,   8.5338,   9.4390,   1.4607,  17.5219,  -5.1740,  -1.1993,\n",
       "          -4.8087,  13.5590], grad_fn=<ViewBackward0>),\n",
       " tensor([[-27.6215, -21.1469, -39.5241,  ...,  -5.8436, -17.4518, -12.8965],\n",
       "         [ 10.0101,   7.1181,  14.3438,  ...,   1.9086,   6.1195,   4.8164],\n",
       "         [ 34.1767,  25.6477,  48.9367,  ...,   7.1494,  21.4964,  15.7157],\n",
       "         ...,\n",
       "         [  3.9620,   3.1727,   5.6136,  ...,   0.8990,   2.4821,   1.8415],\n",
       "         [-17.4784, -13.3993, -24.8015,  ...,  -3.6766, -10.8951,  -8.0058],\n",
       "         [ 29.9768,  23.1130,  42.4060,  ...,   6.3943,  18.7688,  13.7403]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([-52.7694,  18.1431,  65.0214, -87.5992,  18.1664, -32.0867,  57.0518,\n",
       "          25.7244, -80.4705,  -7.6006,   3.5638,  46.6503, -48.8280,  28.1482,\n",
       "         -21.3147, -30.0304, -70.4492,  73.3920,  24.0141, -18.9607,  84.0051,\n",
       "          83.7464,  13.8897, -80.0567, -82.7275, -70.2126,  91.0241, -63.2668,\n",
       "          69.9357,  16.1860,  95.7293,  22.7179, -28.8307,  33.0253, -35.0222,\n",
       "          64.8232, -15.4687,  13.3045,  -2.4490,  96.3805,  87.2227, -50.5666,\n",
       "         -74.1924,  19.7239, -19.7600,  76.2312,  45.8686,  15.9389, -98.5955,\n",
       "         -21.2567,  74.1297, -73.7485, -62.9276, -62.6303, -54.1055,  21.6349,\n",
       "          67.4079,  20.2627, -93.2062, -57.6196,  -7.9063,   7.6901, -33.4344,\n",
       "          57.3395], grad_fn=<ViewBackward0>),\n",
       " tensor([ 2.9302e+01, -2.3300e+01, -3.5283e+01,  3.3825e+01, -6.3794e+00,\n",
       "          1.7427e+01, -5.7620e+01, -3.4017e+00, -6.4188e+00,  2.0513e+00,\n",
       "         -1.4392e+01, -1.9643e+01, -3.9297e+00, -2.7939e+00, -1.3175e-01,\n",
       "          3.3120e+01,  2.4156e+01, -5.3528e+01, -6.4985e+00,  8.0447e+00,\n",
       "         -7.0518e+00, -5.2876e+00, -2.5258e+00,  1.4013e+01, -2.9422e+01,\n",
       "         -2.0285e+01,  1.0956e+01, -5.4116e+01, -2.4497e+01,  2.0601e+01,\n",
       "         -5.4660e+01,  1.2261e+01,  1.0126e+01,  2.6024e+01,  2.8429e+01,\n",
       "          6.0350e+01,  9.0710e+00,  2.1357e+01,  2.4506e-01,  9.1503e+01,\n",
       "         -6.5345e+01, -7.6962e+01,  3.0259e+01,  8.4130e+00, -5.4012e-02,\n",
       "          8.8587e+01,  1.0333e+00,  1.5436e+01,  2.0012e+01, -5.6784e+00,\n",
       "         -6.8005e+01, -9.3926e+01,  2.5555e+01, -6.3712e+01,  7.6108e+01,\n",
       "          1.5547e+01, -4.0236e+01,  4.2749e+01,  2.5520e+01, -5.9202e+01,\n",
       "          1.3539e+01,  8.2692e-01,  2.6193e+01,  7.8064e+01],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([ -47.7105,   22.9784,   66.2096,  -90.4253,   12.4875,  -20.5976,\n",
       "           52.0779,   26.4745,  -96.8415,   -4.3713,   18.6812,   41.8411,\n",
       "          -34.0225,   14.7615,  -12.1912,  -30.8996,  -52.8657,   60.1514,\n",
       "           32.3343,  -14.4993,   81.0278,   81.7977,   -2.6073,  -82.4688,\n",
       "          -88.1256,  -84.3879,   86.5880,  -48.5068,   64.1071,   10.6544,\n",
       "          100.6583,   17.6668,  -42.5850,   38.2379,  -33.6024,   68.7694,\n",
       "          -20.8668,   28.0098,   -3.1046,  118.9984,   75.2208,  -55.9618,\n",
       "          -79.8500,    8.6248,  -33.3845,   69.1526,   60.9940,   18.9473,\n",
       "         -110.0472,  -18.1667,   90.5333,  -82.1100,  -51.1567,  -71.5138,\n",
       "          -59.6948,   18.7371,   83.5151,   32.4241,  -97.5779,  -67.8987,\n",
       "          -19.7375,    1.1278,  -43.9545,   69.0615],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([ 24.2402, -23.7201, -20.9406,  14.8465, -16.1388,  25.3187, -56.9687,\n",
       "         -10.7087,  18.3097,   5.9133,  -1.6420, -31.6240,  -8.3245,  -4.6145,\n",
       "           0.9442,  38.0978,  38.5672, -59.5595,  -6.2894,  13.2880, -17.8530,\n",
       "         -15.5165,   8.0719,   1.6639, -23.4110,  -8.6710,  23.5884, -71.1189,\n",
       "         -14.2357,  28.1160, -66.2578,  12.0874,   6.1172,  15.7566,  23.8483,\n",
       "          62.7114,   9.1825,   5.6171,   0.6376,  71.2127, -84.9762, -83.9447,\n",
       "          56.9450,  17.8212,  -4.6260, 109.0161,   0.8780,  14.9122,  23.8354,\n",
       "          -4.2893, -67.5613, -82.7566,  21.9419, -47.2480,  41.6706,  23.3133,\n",
       "         -36.9860,  27.4235,  27.0489, -65.4897,   5.9562,   4.3590,  24.2989,\n",
       "          74.9359], grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([ -55.9855,   19.3038,   67.7410,  -92.2772,   20.1760,  -33.7197,\n",
       "           60.1803,   26.5373,  -84.8759,   -8.3334,    3.0507,   48.3235,\n",
       "          -52.4504,   28.1519,  -23.1286,  -33.2306,  -72.0769,   73.7685,\n",
       "           25.2812,  -20.4050,   86.7691,   85.9397,   13.6103,  -82.7592,\n",
       "          -85.0706,  -72.1738,   93.4526,  -65.7640,   71.9277,   15.3220,\n",
       "           98.3524,   22.2615,  -30.4144,   33.1140,  -36.4077,   66.7369,\n",
       "          -16.2854,   13.8943,   -2.7035,   98.8977,   90.1377,  -52.8782,\n",
       "          -76.5535,   20.3504,  -20.9450,   77.5493,   47.0417,   16.2594,\n",
       "         -101.7639,  -22.5705,   76.6870,  -76.1810,  -64.5362,  -64.9955,\n",
       "          -56.1546,   22.0030,   70.0952,   20.0590,  -96.2282,  -59.7047,\n",
       "           -8.4544,    7.7821,  -34.4538,   59.0451],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000],\n",
       "         [ -357.8239, -1015.5087,  -255.4738,  -132.9655,  -661.0648,  -620.5347,\n",
       "           -782.3305,  -333.4948,  -178.2808,  -586.4276,  -444.8063,  -540.8380,\n",
       "            131.1654,  -135.4654,   -33.7386,  -947.4818,  -442.2123,  -667.2500,\n",
       "           -205.5987,  -538.1839,  -170.0410,  -149.2140,   490.1361,   -16.6158,\n",
       "            227.4303,    99.2887,   208.6009,   893.7277,  -163.5655,  1516.5151,\n",
       "           -556.7501,   448.7328,  -166.2204,   393.2417,  -541.3427,   776.5844,\n",
       "           -465.9823,   334.1042,  -194.9146,   595.0859,  -779.1113,  1311.9724,\n",
       "           -614.7505,   723.7212,   182.5284,  1161.7733,    15.4252,   757.9580,\n",
       "           -193.5695,   157.0544,  -728.0889,   897.7690,  -280.9824,   600.7711,\n",
       "           -613.2711,   875.6465,  -436.0715,  1129.8540,  -232.3039,   906.5109,\n",
       "           -582.2330,   462.9093,  -582.8511,  1048.8524]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([  0.0000, 826.4346], grad_fn=<ViewBackward0>),\n",
       " tensor([[-6.7051e-06, -1.0191e-05,  7.7580e-06,  5.3934e-07, -5.7575e-06,\n",
       "          -2.8175e-06, -6.0029e-06,  5.3669e-06,  7.0394e-06,  4.6851e-06,\n",
       "          -1.0377e-05,  5.8817e-07,  2.2992e-06, -1.4136e-06, -3.2814e-07,\n",
       "           1.4092e-06, -6.7020e-06,  9.2760e-07, -5.6188e-06, -2.2700e-06,\n",
       "          -2.7499e-06, -4.8439e-06,  4.0595e-06, -1.5966e-06,  8.0923e-07,\n",
       "          -2.1833e-06,  2.0483e-06,  2.7520e-06, -1.0591e-06,  7.5670e-06,\n",
       "          -1.6073e-06,  3.7737e-06,  2.2574e-06,  3.0505e-06, -1.2083e-06,\n",
       "           1.6781e-06, -3.3071e-07,  8.3305e-07,  8.9883e-08,  1.7388e-06,\n",
       "          -4.1976e-06,  6.1112e-06, -2.7063e-06,  1.1631e-06,  1.5247e-06,\n",
       "           5.5730e-06,  7.2189e-08,  1.4969e-06,  2.0418e-07,  8.5851e-07,\n",
       "          -4.9751e-06,  2.1227e-06, -1.0197e-06,  3.6433e-06, -1.4470e-06,\n",
       "           2.2661e-06, -3.6277e-06,  5.1757e-06, -1.1813e-06,  3.7428e-06,\n",
       "          -2.2552e-06,  5.9197e-07, -2.5644e-06,  1.9196e-06]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([3.3159e-06], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ln_P_d_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(psi_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "print((P := psi_x * psi_x.conj()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5769-6.1739e-01j, -0.6152+8.4863e-01j],\n",
       "        [ 1.7631+7.5909e-01j,  0.1033+1.3899e+00j],\n",
       "        [ 0.3123-2.5167e-03j,  0.0912-1.1067e+00j],\n",
       "        [ 0.9518-1.4823e+00j, -0.5423+4.7282e-01j],\n",
       "        [ 1.5811+1.1638e+00j,  0.5020+2.9848e+00j],\n",
       "        [-1.6522-1.4240e+00j, -0.2496-1.8722e-01j],\n",
       "        [-1.4371-4.0499e-01j,  1.7068+1.1778e+00j],\n",
       "        [-1.1488+1.8205e+00j, -0.1983-1.2034e+00j],\n",
       "        [ 0.2536+7.6231e-01j,  0.9571+1.1045e+00j],\n",
       "        [ 1.5606+4.7761e-01j,  0.1221-2.1698e-01j]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_re = torch.randn(10, 2)\n",
    "ex_im = torch.randn(10, 2)\n",
    "ex_com = torch.complex(ex_re, ex_im)\n",
    "ex_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1365, 4.6943])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_com.norm(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape is psi_x * psi_x.conj()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n",
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "print(psi_x.shape)\n",
    "print((psi_x * psi_x.conj()).shape)  # This produces a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3270, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, -2.3047,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.4629,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  5.6694,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  0.0000,  1.4428]]),)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import vmap\n",
    "\n",
    "N = 5\n",
    "M = 12  # Batch dimension\n",
    "x = torch.randn(N, requires_grad=True)\n",
    "y = torch.randn(N, requires_grad=True)\n",
    "f = lambda x, y: x**2 + y**2\n",
    "z = f(x, y)\n",
    "print(z.shape)\n",
    "\n",
    "\n",
    "def get_vjp(v):\n",
    "    return torch.autograd.grad(z, x, grad_outputs=v, retain_graph=True)\n",
    "\n",
    "\n",
    "(jacobian := torch.vmap(get_vjp, randomness=\"same\")(torch.eye(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[641], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vjp\u001b[39m(v):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m (jacobian \u001b[38;5;241m:=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvmap(get_vjp, in_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, randomness\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(torch\u001b[38;5;241m.\u001b[39meye(M)))\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    282\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[0;32mIn[641], line 11\u001b[0m, in \u001b[0;36mget_vjp\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vjp\u001b[39m(v):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _make_grads(\n\u001b[1;32m    386\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:82\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     80\u001b[0m     shape_matches \u001b[38;5;241m=\u001b[39m expect_true(sym_eq(out\u001b[38;5;241m.\u001b[39msize(), first_grad\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_matches:\n\u001b[0;32m---> 82\u001b[0m     out_shape, grad_shape \u001b[38;5;241m=\u001b[39m _calculate_shape(\n\u001b[1;32m     83\u001b[0m         out, first_grad, is_grads_batched\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:45\u001b[0m, in \u001b[0;36m_calculate_shape\u001b[0;34m(output, grad, is_grads_batched)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_shape\u001b[39m(\n\u001b[1;32m     41\u001b[0m     output: torch\u001b[38;5;241m.\u001b[39mTensor, grad: torch\u001b[38;5;241m.\u001b[39mTensor, is_grads_batched: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_ShapeorNestedShape, _ShapeorNestedShape]:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# is_same_size ensures that both tensors are either nested or non nested\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# circular import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NestedTensor\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, NestedTensor):\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:416\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ret_nt, offsets, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_contiguous \u001b[38;5;28;01melse\u001b[39;00m length_list)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# NB: A dummy arg is required so that NestedTensor.__torch_dispatch__() is invoked\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# for _nested_view_from_values_offsets(). Sizes don't matter much, but they shouldn't be\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# 0/1 because the dummy can be fake-ified and we want to avoid specializing.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# This arg is otherwise unused.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m _nt_view_dummy \u001b[38;5;241m=\u001b[39m NestedTensor(\n\u001b[1;32m    417\u001b[0m     values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    418\u001b[0m     offsets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    419\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnested_view_from_values_offsets\u001b[39m(values, offsets, ragged_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_nested_view_from_jagged(\n\u001b[1;32m    424\u001b[0m         values, offsets, _nt_view_dummy, \u001b[38;5;28;01mNone\u001b[39;00m, ragged_idx\n\u001b[1;32m    425\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:112\u001b[0m, in \u001b[0;36mNestedTensor.__init__\u001b[0;34m(self, values, offsets, lengths, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_cache \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# collapsed ragged dim must always be dynamic\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx)\n\u001b[1;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:223\u001b[0m, in \u001b[0;36mmark_dynamic\u001b[0;34m(t, index, min, max)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mMark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_traceable_wrapper_subclass(t):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# TODO: Make this configurable via a supported public API\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     _apply_func_to_inner_tensors_of_same_dim(\n\u001b[1;32m    224\u001b[0m         mark_dynamic, t, index, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dynamo_dynamic_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:178\u001b[0m, in \u001b[0;36m_apply_func_to_inner_tensors_of_same_dim\u001b[0;34m(func, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    177\u001b[0m     inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(t, attr)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inner\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39mdim():\n\u001b[1;32m    179\u001b[0m         func(inner, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:232\u001b[0m, in \u001b[0;36mNestedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:216\u001b[0m, in \u001b[0;36mNestedTensor.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m fn \u001b[38;5;241m=\u001b[39m lookup_jagged(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(func)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:181\u001b[0m, in \u001b[0;36mregister_func.<locals>.wrapper.<locals>.get_inner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 181\u001b[0m     check_schema(schema_str, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(aten_op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:118\u001b[0m, in \u001b[0;36mcheck_schema\u001b[0;34m(schema_str, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_fn(args[i]):\n\u001b[1;32m    110\u001b[0m     type_to_desc \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt?\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptional tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<any type>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     }\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNestedTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_to_desc[arg_type]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "M = 12  # Batch dimension\n",
    "x = torch.randn((N, M), requires_grad=True)\n",
    "y = torch.randn((N, M), requires_grad=True)\n",
    "f = lambda x, y: x**2 + y**2\n",
    "z = f(x, y)\n",
    "print(z.shape)\n",
    "\n",
    "\n",
    "def get_vjp(v):\n",
    "    return torch.autograd.grad(z, x, grad_outputs=v, retain_graph=True)\n",
    "\n",
    "\n",
    "(jacobian := torch.vmap(get_vjp, in_dims=1, randomness=\"same\")(torch.eye(M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1966, -0.7046,  0.8144])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.dot has type [D], [D] -> []\n",
    "\n",
    "# What if we wanted to apply it across a batch dimension where the second dim is the batch dim? [N, D], [N, D] -> [D]\n",
    "\n",
    "batched_dot = vmap(torch.dot, in_dims=1)\n",
    "x, y = torch.randn(5, 3), torch.randn(5, 3)\n",
    "batched_dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to map torch.autograd.grad over a batch of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[648], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m grad_vmap \u001b[38;5;241m=\u001b[39m vmap(grad_on_seq, in_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, randomness\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m grad_vmap(torch\u001b[38;5;241m.\u001b[39meye(N))\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    282\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[0;32mIn[648], line 4\u001b[0m, in \u001b[0;36mgrad_on_seq\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_on_seq\u001b[39m(v):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _make_grads(\n\u001b[1;32m    386\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:82\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     80\u001b[0m     shape_matches \u001b[38;5;241m=\u001b[39m expect_true(sym_eq(out\u001b[38;5;241m.\u001b[39msize(), first_grad\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_matches:\n\u001b[0;32m---> 82\u001b[0m     out_shape, grad_shape \u001b[38;5;241m=\u001b[39m _calculate_shape(\n\u001b[1;32m     83\u001b[0m         out, first_grad, is_grads_batched\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:45\u001b[0m, in \u001b[0;36m_calculate_shape\u001b[0;34m(output, grad, is_grads_batched)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_shape\u001b[39m(\n\u001b[1;32m     41\u001b[0m     output: torch\u001b[38;5;241m.\u001b[39mTensor, grad: torch\u001b[38;5;241m.\u001b[39mTensor, is_grads_batched: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_ShapeorNestedShape, _ShapeorNestedShape]:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# is_same_size ensures that both tensors are either nested or non nested\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# circular import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NestedTensor\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, NestedTensor):\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:416\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ret_nt, offsets, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_contiguous \u001b[38;5;28;01melse\u001b[39;00m length_list)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# NB: A dummy arg is required so that NestedTensor.__torch_dispatch__() is invoked\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# for _nested_view_from_values_offsets(). Sizes don't matter much, but they shouldn't be\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# 0/1 because the dummy can be fake-ified and we want to avoid specializing.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# This arg is otherwise unused.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m _nt_view_dummy \u001b[38;5;241m=\u001b[39m NestedTensor(\n\u001b[1;32m    417\u001b[0m     values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    418\u001b[0m     offsets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    419\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnested_view_from_values_offsets\u001b[39m(values, offsets, ragged_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_nested_view_from_jagged(\n\u001b[1;32m    424\u001b[0m         values, offsets, _nt_view_dummy, \u001b[38;5;28;01mNone\u001b[39;00m, ragged_idx\n\u001b[1;32m    425\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:112\u001b[0m, in \u001b[0;36mNestedTensor.__init__\u001b[0;34m(self, values, offsets, lengths, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_cache \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# collapsed ragged dim must always be dynamic\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx)\n\u001b[1;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:223\u001b[0m, in \u001b[0;36mmark_dynamic\u001b[0;34m(t, index, min, max)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mMark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_traceable_wrapper_subclass(t):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# TODO: Make this configurable via a supported public API\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     _apply_func_to_inner_tensors_of_same_dim(\n\u001b[1;32m    224\u001b[0m         mark_dynamic, t, index, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dynamo_dynamic_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:178\u001b[0m, in \u001b[0;36m_apply_func_to_inner_tensors_of_same_dim\u001b[0;34m(func, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    177\u001b[0m     inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(t, attr)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inner\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39mdim():\n\u001b[1;32m    179\u001b[0m         func(inner, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:232\u001b[0m, in \u001b[0;36mNestedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:216\u001b[0m, in \u001b[0;36mNestedTensor.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m fn \u001b[38;5;241m=\u001b[39m lookup_jagged(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(func)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:181\u001b[0m, in \u001b[0;36mregister_func.<locals>.wrapper.<locals>.get_inner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 181\u001b[0m     check_schema(schema_str, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(aten_op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:118\u001b[0m, in \u001b[0;36mcheck_schema\u001b[0;34m(schema_str, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_fn(args[i]):\n\u001b[1;32m    110\u001b[0m     type_to_desc \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt?\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptional tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<any type>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     }\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNestedTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_to_desc[arg_type]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor"
     ]
    }
   ],
   "source": [
    "# A function that takes in a sample of dimension (N,) and returns the gradient with respect to\n",
    "# x\n",
    "def grad_on_seq(v):\n",
    "    return torch.autograd.grad(z, x, grad_outputs=v, retain_graph=True)\n",
    "\n",
    "\n",
    "grad_vmap = vmap(grad_on_seq, in_dims=1, randomness=\"same\")\n",
    "grad_vmap(torch.eye(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
