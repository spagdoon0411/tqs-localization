{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TQS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64\n",
    "MAX_LENGTH = 100\n",
    "NUM_HEADS = 1\n",
    "NUM_LAYERS = 1\n",
    "DIM_FEEDFORWARD = 128\n",
    "TEST_LENGTH = 50\n",
    "TEST_BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_potentials = torch.randn(TEST_LENGTH, TEST_BATCH)\n",
    "test_spins = torch.zeros(TEST_LENGTH, TEST_BATCH)\n",
    "test_spins[torch.randint(0, TEST_LENGTH, (TEST_BATCH,)), torch.arange(TEST_BATCH)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spandan/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "tqs = TQS(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    max_chain_len=MAX_LENGTH,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    possible_spins=2,\n",
    "    dim_feedforward=DIM_FEEDFORWARD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, phases = tqs(test_potentials, test_spins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4685, 0.5502],\n",
       "         [0.4831, 0.5676],\n",
       "         [0.3663, 0.5296],\n",
       "         ...,\n",
       "         [0.4775, 0.5025],\n",
       "         [0.4807, 0.5330],\n",
       "         [0.3822, 0.5043]],\n",
       "\n",
       "        [[0.5155, 0.5580],\n",
       "         [0.4286, 0.5356],\n",
       "         [0.3694, 0.3971],\n",
       "         ...,\n",
       "         [0.3951, 0.4800],\n",
       "         [0.3627, 0.4351],\n",
       "         [0.3857, 0.4661]],\n",
       "\n",
       "        [[0.3079, 0.4326],\n",
       "         [0.3701, 0.4135],\n",
       "         [0.3565, 0.4776],\n",
       "         ...,\n",
       "         [0.4106, 0.4472],\n",
       "         [0.3604, 0.4803],\n",
       "         [0.4186, 0.5195]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.1973, 0.5313],\n",
       "         [0.2029, 0.5303],\n",
       "         [0.2131, 0.5967],\n",
       "         ...,\n",
       "         [0.2172, 0.5763],\n",
       "         [0.2065, 0.4921],\n",
       "         [0.1831, 0.5414]],\n",
       "\n",
       "        [[0.2584, 0.5622],\n",
       "         [0.2390, 0.5443],\n",
       "         [0.2392, 0.5655],\n",
       "         ...,\n",
       "         [0.2285, 0.5596],\n",
       "         [0.2363, 0.5448],\n",
       "         [0.2393, 0.5657]],\n",
       "\n",
       "        [[0.2659, 0.5688],\n",
       "         [0.2887, 0.5277],\n",
       "         [0.2696, 0.5833],\n",
       "         ...,\n",
       "         [0.2996, 0.5636],\n",
       "         [0.3070, 0.5074],\n",
       "         [0.3075, 0.5442]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def autoregressive_sample(model, initial_potentials, max_length):\n",
    "    model.eval()\n",
    "    # Allocate a buffer for the sampled basis states\n",
    "    sampled_spins = torch.zeros(max_length, initial_potentials.size(1))\n",
    "    batch_size = initial_potentials.size(1)\n",
    "    batch_remaining_idx = torch.arange(batch_size)\n",
    "\n",
    "    for i in range(max_length):\n",
    "        # get P(s_{i+1} | V, s_{1:i}) and phi(s_{i+1} | V, s_{1:i}) distributions\n",
    "        probs, _ = model(\n",
    "            initial_potentials[:, batch_remaining_idx],\n",
    "            sampled_spins[:i, batch_remaining_idx],\n",
    "        )\n",
    "\n",
    "        # sample s_{i+1} from P(s_{i+1} | V, s_{1:i})\n",
    "        last_probs = probs[-1]  # (batch, 2); P(s_{i+1} | V, s_{1:i})\n",
    "        sampled_spins[i, batch_remaining_idx] = (\n",
    "            torch.multinomial(last_probs, 1).squeeze().float()\n",
    "        )\n",
    "\n",
    "        # a mask with dimension (batch_remaining,); True if we sampled a 1\n",
    "        newly_completed = sampled_spins[i, batch_remaining_idx] == 1.0\n",
    "\n",
    "        # mask out the batch_remaining_idx that have been completed\n",
    "        batch_remaining_idx = batch_remaining_idx[~newly_completed]\n",
    "\n",
    "    return sampled_spins\n",
    "\n",
    "\n",
    "sampled_spins = autoregressive_sample(tqs, test_potentials, TEST_LENGTH)\n",
    "sampled_spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_spins.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_spins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_spins = tqs.sample_spins(test_potentials, TEST_LENGTH)\n",
    "sampled_spins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    sampled_spins.sum(dim=0) == 1.0\n",
    ").all()  # each batch should have exactly one 1 spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "T = 1.0\n",
    "print(tqs.compute_psi(sampled_spins, test_potentials, T).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.6965-0.1732j,  0.2971-0.6767j,  0.2579-0.6938j,  ...,\n",
       "           0.3527-0.6484j,  0.7091-0.1241j,  0.6904-0.2013j],\n",
       "         [ 0.4320-0.5739j,  0.6759+0.0923j,  0.6806-0.0020j,  ...,\n",
       "           0.6415+0.2341j,  0.4454-0.5647j,  0.4077-0.5937j],\n",
       "         [ 0.6814-0.1205j,  0.6776-0.1526j,  0.6497-0.2423j,  ...,\n",
       "           0.6965+0.0055j,  0.6930-0.0560j,  0.6752-0.1600j],\n",
       "         ...,\n",
       "         [-0.5646+0.4825j, -0.5629+0.4853j, -0.5520+0.4984j,  ...,\n",
       "          -0.5897+0.4516j, -0.5793+0.4649j, -0.5622+0.4862j],\n",
       "         [-0.4920+0.5745j, -0.4923+0.5744j, -0.4857+0.5797j,  ...,\n",
       "          -0.5279+0.5422j, -0.5137+0.5558j, -0.4917+0.5749j],\n",
       "         [-0.4723+0.5699j, -0.4764+0.5660j, -0.4802+0.5628j,  ...,\n",
       "          -0.5119+0.5360j, -0.4970+0.5496j, -0.4755+0.5667j]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[ 0.6965-0.1739j,  0.6904-0.2015j,  0.6652-0.2727j,  ...,\n",
       "           0.7163-0.0793j,  0.7092-0.1250j,  0.6917-0.1954j],\n",
       "         [ 0.6675+0.1256j,  0.4044-0.5963j,  0.3621-0.6252j,  ...,\n",
       "           0.4662-0.5471j,  0.6574+0.1834j,  0.6742+0.0926j],\n",
       "         [ 0.3534-0.6329j,  0.6761-0.1564j,  0.6494-0.2433j,  ...,\n",
       "           0.6955-0.0035j,  0.3633-0.6291j,  0.3309-0.6468j],\n",
       "         ...,\n",
       "         [-0.5632+0.4843j, -0.5617+0.4866j, -0.5528+0.4974j,  ...,\n",
       "          -0.5885+0.4530j, -0.5799+0.4643j, -0.5626+0.4854j],\n",
       "         [-0.4905+0.5757j, -0.4912+0.5753j, -0.4869+0.5787j,  ...,\n",
       "          -0.5262+0.5438j, -0.5146+0.5550j, -0.4921+0.5745j],\n",
       "         [-0.4713+0.5706j, -0.4758+0.5666j, -0.4817+0.5616j,  ...,\n",
       "          -0.5101+0.5377j, -0.4976+0.5490j, -0.4755+0.5670j]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[ 0.3203-0.6644j,  0.6902-0.2034j,  0.6663-0.2688j,  ...,\n",
       "           0.7169-0.0806j,  0.3329-0.6585j,  0.2991-0.6754j],\n",
       "         [ 0.6683+0.1223j,  0.6761+0.0916j,  0.6800+0.0036j,  ...,\n",
       "           0.6421+0.2337j,  0.6596+0.1727j,  0.6765+0.0803j],\n",
       "         [ 0.6817-0.1211j,  0.6785-0.1511j,  0.6511-0.2372j,  ...,\n",
       "           0.6971+0.0095j,  0.6916-0.0682j,  0.6740-0.1657j],\n",
       "         ...,\n",
       "         [-0.5659+0.4810j, -0.5629+0.4857j, -0.5510+0.4995j,  ...,\n",
       "          -0.5903+0.4512j, -0.5781+0.4663j, -0.5614+0.4872j],\n",
       "         [-0.4934+0.5734j, -0.4921+0.5746j, -0.4841+0.5810j,  ...,\n",
       "          -0.5286+0.5416j, -0.5129+0.5565j, -0.4906+0.5758j],\n",
       "         [-0.4728+0.5694j,  0.6988+0.3063j,  0.6998+0.3040j,  ...,\n",
       "           0.6814+0.3415j, -0.4975+0.5491j, -0.4746+0.5672j]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x, psi_l, psi_r = tqs.psi_terms(sampled_spins, test_potentials, T)\n",
    "assert (\n",
    "    psi_x.shape == psi_l.shape == psi_r.shape == torch.Size([TEST_LENGTH, TEST_BATCH])\n",
    ")\n",
    "psi_x, psi_l, psi_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7186+0.8070j, -1.7591-0.3370j, -1.2952-1.4781j,  ...,\n",
       "         -1.3619-1.1539j, -2.2010+0.9252j, -1.5639+0.7484j],\n",
       "        [-1.6782-0.5832j, -1.9106+0.8872j, -1.1814+0.9077j,  ...,\n",
       "         -1.5238+0.9304j,  0.0401-2.7400j, -0.1941-2.6570j],\n",
       "        [-1.1188+0.7164j, -1.1177-0.1952j, -2.0296+0.0053j,  ...,\n",
       "          0.2159+0.0247j, -1.5791+0.8763j, -2.2518+0.9412j],\n",
       "        ...,\n",
       "        [-1.9615-0.0327j, -2.2533+0.2210j, -2.5262+0.4753j,  ...,\n",
       "         -2.1406+0.1091j, -1.9447-0.0432j, -2.6785+0.5875j],\n",
       "        [-1.2037-0.9294j, -1.9260-0.0843j, -1.6937-0.3651j,  ...,\n",
       "         -1.4054-0.6091j, -1.6908-0.3349j, -1.5309-0.5471j],\n",
       "        [-2.3453+0.4176j, -0.8352+1.1407j, -0.2810+0.4942j,  ...,\n",
       "         -0.3876+0.6609j, -2.8730+0.9634j, -0.9564-1.2422j]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_loc = tqs.E_loc(psi_x, psi_l, psi_r, test_potentials, T)\n",
    "E_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6965-0.1732j,  0.2971-0.6767j,  0.2579-0.6938j,  ...,\n",
       "          0.3527-0.6484j,  0.7091-0.1241j,  0.6904-0.2013j],\n",
       "        [ 0.4320-0.5739j,  0.6759+0.0923j,  0.6806-0.0020j,  ...,\n",
       "          0.6415+0.2341j,  0.4454-0.5647j,  0.4077-0.5937j],\n",
       "        [ 0.6814-0.1205j,  0.6776-0.1526j,  0.6497-0.2423j,  ...,\n",
       "          0.6965+0.0055j,  0.6930-0.0560j,  0.6752-0.1600j],\n",
       "        ...,\n",
       "        [-0.5646+0.4825j, -0.5629+0.4853j, -0.5520+0.4984j,  ...,\n",
       "         -0.5897+0.4516j, -0.5793+0.4649j, -0.5622+0.4862j],\n",
       "        [-0.4920+0.5745j, -0.4923+0.5744j, -0.4857+0.5797j,  ...,\n",
       "         -0.5279+0.5422j, -0.5137+0.5558j, -0.4917+0.5749j],\n",
       "        [-0.4723+0.5699j, -0.4764+0.5660j, -0.4802+0.5628j,  ...,\n",
       "         -0.5119+0.5360j, -0.4970+0.5496j, -0.4755+0.5667j]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.4510,  1.7612,  0.0871,  0.2424,  0.1194,  0.0306, -0.7946,  0.1000,\n",
      "        -0.6361, -0.7865,  1.0720, -1.7430, -0.6640,  0.5250,  0.7989, -0.8286,\n",
      "         0.1016, -0.7357, -1.6222,  0.6305,  0.8901,  0.1104,  1.0467, -0.9099,\n",
      "         0.8417, -0.0781,  0.5098,  0.2046, -0.9705, -0.2621,  0.0456, -1.3932,\n",
      "         1.7074,  0.4026,  2.1397,  0.2063,  0.9070,  0.9167,  0.0660,  1.1285,\n",
      "         0.6552,  1.6142,  0.3154, -0.4186, -1.9631, -2.1122, -0.5247,  1.4032,\n",
      "         0.8821,  1.0708, -1.1718,  1.0588,  0.9746, -0.1113, -0.9600,  1.8159,\n",
      "        -0.5182,  0.1059, -0.3379,  0.5837, -1.3769, -0.7077,  0.7027, -0.0786],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0074,  0.3213, -1.0453, -0.0669,  0.3513, -0.2405, -1.9639,  1.1031,\n",
      "         2.2512,  0.2668,  0.4270,  0.3283,  0.5749,  1.2590, -1.2979, -2.1353,\n",
      "         0.3623,  0.3440,  0.9656, -1.2559,  0.3636,  1.7245,  0.2520,  0.3070,\n",
      "         0.8970,  0.2844,  1.0224, -0.0115,  0.4861, -0.3954,  0.1423,  0.4674,\n",
      "         0.3549, -1.1678,  0.5129,  0.6797, -1.0458, -0.6931,  0.3974,  0.9238,\n",
      "        -0.2534, -0.7657,  1.1691,  1.4217,  0.0277,  1.8423,  1.2249, -0.4188,\n",
      "        -0.8852,  0.6921, -0.0349,  0.3366,  1.7610,  0.7109, -0.8025,  1.4084,\n",
      "         0.2527,  0.9761,  0.6506,  0.5439, -0.9131, -1.2482,  0.6361,  0.7893],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0142, -0.1206, -0.0085,  ...,  0.0356,  0.1036,  0.0668],\n",
      "        [ 0.1433, -0.1121,  0.0466,  ..., -0.1290, -0.0176,  0.0573],\n",
      "        [-0.1088, -0.0660, -0.0373,  ...,  0.0722, -0.0821, -0.0978],\n",
      "        ...,\n",
      "        [ 0.0698,  0.1171, -0.1345,  ..., -0.0748, -0.0094, -0.1493],\n",
      "        [-0.0763, -0.0410, -0.1077,  ..., -0.0613,  0.0877,  0.0401],\n",
      "        [ 0.0734,  0.1390, -0.0461,  ...,  0.0982, -0.0334,  0.1394]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1052, -0.0879,  0.0100,  ...,  0.1159, -0.0250,  0.0060],\n",
      "        [ 0.0748, -0.0401,  0.1012,  ...,  0.0213,  0.0995,  0.0915],\n",
      "        [ 0.0301, -0.0842,  0.0019,  ..., -0.0437,  0.0655,  0.0474],\n",
      "        ...,\n",
      "        [ 0.0054, -0.0008, -0.1081,  ..., -0.1123,  0.0896,  0.0828],\n",
      "        [ 0.0261, -0.0780,  0.1058,  ...,  0.0799, -0.1083,  0.0338],\n",
      "        [-0.1022,  0.0762, -0.0471,  ...,  0.0934, -0.0243, -0.1099]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0416, -0.0949, -0.0738,  ..., -0.0579,  0.1113,  0.0938],\n",
      "        [-0.0231, -0.0330, -0.0330,  ...,  0.0850,  0.0251, -0.0446],\n",
      "        [-0.0597,  0.0421, -0.0929,  ...,  0.0973,  0.0914, -0.0207],\n",
      "        ...,\n",
      "        [ 0.0241, -0.0866, -0.1187,  ...,  0.1010,  0.0373,  0.1103],\n",
      "        [ 0.0773, -0.0537, -0.1116,  ...,  0.0059, -0.0886,  0.0019],\n",
      "        [ 0.0361,  0.0084, -0.0369,  ..., -0.1090,  0.1163, -0.0103]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0393,  0.1073, -0.0941, -0.0869,  0.0924, -0.0129, -0.1007, -0.0232,\n",
      "        -0.0277, -0.0843, -0.0581, -0.0080, -0.0263,  0.0867, -0.0594,  0.0853,\n",
      "         0.1126,  0.0220,  0.0586,  0.0638, -0.1145,  0.0385, -0.0376, -0.0180,\n",
      "         0.0735,  0.0121, -0.0479, -0.0768,  0.0797, -0.1088, -0.0638,  0.0939,\n",
      "        -0.0556, -0.0014, -0.0703,  0.1027,  0.0156,  0.0830,  0.1199, -0.0789,\n",
      "        -0.0580,  0.0731,  0.0959, -0.0007, -0.0974,  0.0585,  0.0049,  0.1213,\n",
      "        -0.1060,  0.0277, -0.0414,  0.0035,  0.0495,  0.0777,  0.0677, -0.0152,\n",
      "        -0.0618, -0.1181,  0.0912,  0.1013,  0.0122,  0.0712,  0.1173,  0.0535,\n",
      "        -0.0741,  0.0678, -0.0993, -0.0225, -0.0649, -0.0430,  0.0453,  0.0563,\n",
      "         0.0396,  0.0085, -0.0064,  0.0820,  0.0531, -0.0870, -0.0729,  0.0742,\n",
      "        -0.1026, -0.0259,  0.0325, -0.0792, -0.0234,  0.0677, -0.0089,  0.0977,\n",
      "        -0.0952, -0.1158, -0.0709,  0.1008, -0.0098, -0.0789, -0.0203,  0.0047,\n",
      "        -0.0591,  0.0522, -0.0295, -0.0071,  0.0019, -0.0998, -0.0712, -0.0135,\n",
      "        -0.1154,  0.0142,  0.1070,  0.0224, -0.0144, -0.0754,  0.1198, -0.0859,\n",
      "        -0.0794,  0.0065, -0.0134, -0.0152,  0.0463,  0.0728, -0.0163,  0.0868,\n",
      "        -0.0950,  0.0857, -0.0169, -0.0176, -0.1219, -0.0547, -0.0197,  0.0904],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0177,  0.0739, -0.0219,  ..., -0.0744,  0.0301, -0.0184],\n",
      "        [-0.0553,  0.0045, -0.0593,  ...,  0.0023, -0.0078,  0.0262],\n",
      "        [ 0.0576,  0.0006, -0.0151,  ..., -0.0199,  0.0334,  0.0152],\n",
      "        ...,\n",
      "        [-0.0314,  0.0259, -0.0453,  ..., -0.0318,  0.0148, -0.0222],\n",
      "        [-0.0573, -0.0110,  0.0833,  ...,  0.0731, -0.0746,  0.0252],\n",
      "        [ 0.0426,  0.0357, -0.0631,  ..., -0.0156, -0.0460,  0.0144]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0373,  0.0086,  0.0841,  0.0012,  0.0316, -0.0269, -0.0776, -0.0882,\n",
      "        -0.0200, -0.0292, -0.0856, -0.0785,  0.0127, -0.0443, -0.0343,  0.0325,\n",
      "        -0.0661,  0.0137,  0.0612,  0.0477,  0.0603, -0.0182, -0.0009, -0.0663,\n",
      "         0.0445, -0.0342,  0.0291,  0.0721, -0.0315,  0.0498, -0.0106,  0.0223,\n",
      "         0.0728, -0.0187, -0.0227, -0.0358, -0.0667,  0.0753, -0.0042, -0.0806,\n",
      "         0.0043,  0.0066,  0.0144, -0.0467,  0.0098,  0.0187,  0.0812,  0.0471,\n",
      "         0.0455,  0.0400,  0.0429,  0.0837, -0.0458, -0.0691, -0.0019, -0.0078,\n",
      "        -0.0363,  0.0612, -0.0663, -0.0438,  0.0041, -0.0020,  0.0559, -0.0179],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0142, -0.1206, -0.0085,  ...,  0.0356,  0.1036,  0.0668],\n",
      "        [ 0.1433, -0.1121,  0.0466,  ..., -0.1290, -0.0176,  0.0573],\n",
      "        [-0.1088, -0.0660, -0.0373,  ...,  0.0722, -0.0821, -0.0978],\n",
      "        ...,\n",
      "        [ 0.0698,  0.1171, -0.1345,  ..., -0.0748, -0.0094, -0.1493],\n",
      "        [-0.0763, -0.0410, -0.1077,  ..., -0.0613,  0.0877,  0.0401],\n",
      "        [ 0.0734,  0.1390, -0.0461,  ...,  0.0982, -0.0334,  0.1394]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1052, -0.0879,  0.0100,  ...,  0.1159, -0.0250,  0.0060],\n",
      "        [ 0.0748, -0.0401,  0.1012,  ...,  0.0213,  0.0995,  0.0915],\n",
      "        [ 0.0301, -0.0842,  0.0019,  ..., -0.0437,  0.0655,  0.0474],\n",
      "        ...,\n",
      "        [ 0.0054, -0.0008, -0.1081,  ..., -0.1123,  0.0896,  0.0828],\n",
      "        [ 0.0261, -0.0780,  0.1058,  ...,  0.0799, -0.1083,  0.0338],\n",
      "        [-0.1022,  0.0762, -0.0471,  ...,  0.0934, -0.0243, -0.1099]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0416, -0.0949, -0.0738,  ..., -0.0579,  0.1113,  0.0938],\n",
      "        [-0.0231, -0.0330, -0.0330,  ...,  0.0850,  0.0251, -0.0446],\n",
      "        [-0.0597,  0.0421, -0.0929,  ...,  0.0973,  0.0914, -0.0207],\n",
      "        ...,\n",
      "        [ 0.0241, -0.0866, -0.1187,  ...,  0.1010,  0.0373,  0.1103],\n",
      "        [ 0.0773, -0.0537, -0.1116,  ...,  0.0059, -0.0886,  0.0019],\n",
      "        [ 0.0361,  0.0084, -0.0369,  ..., -0.1090,  0.1163, -0.0103]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0393,  0.1073, -0.0941, -0.0869,  0.0924, -0.0129, -0.1007, -0.0232,\n",
      "        -0.0277, -0.0843, -0.0581, -0.0080, -0.0263,  0.0867, -0.0594,  0.0853,\n",
      "         0.1126,  0.0220,  0.0586,  0.0638, -0.1145,  0.0385, -0.0376, -0.0180,\n",
      "         0.0735,  0.0121, -0.0479, -0.0768,  0.0797, -0.1088, -0.0638,  0.0939,\n",
      "        -0.0556, -0.0014, -0.0703,  0.1027,  0.0156,  0.0830,  0.1199, -0.0789,\n",
      "        -0.0580,  0.0731,  0.0959, -0.0007, -0.0974,  0.0585,  0.0049,  0.1213,\n",
      "        -0.1060,  0.0277, -0.0414,  0.0035,  0.0495,  0.0777,  0.0677, -0.0152,\n",
      "        -0.0618, -0.1181,  0.0912,  0.1013,  0.0122,  0.0712,  0.1173,  0.0535,\n",
      "        -0.0741,  0.0678, -0.0993, -0.0225, -0.0649, -0.0430,  0.0453,  0.0563,\n",
      "         0.0396,  0.0085, -0.0064,  0.0820,  0.0531, -0.0870, -0.0729,  0.0742,\n",
      "        -0.1026, -0.0259,  0.0325, -0.0792, -0.0234,  0.0677, -0.0089,  0.0977,\n",
      "        -0.0952, -0.1158, -0.0709,  0.1008, -0.0098, -0.0789, -0.0203,  0.0047,\n",
      "        -0.0591,  0.0522, -0.0295, -0.0071,  0.0019, -0.0998, -0.0712, -0.0135,\n",
      "        -0.1154,  0.0142,  0.1070,  0.0224, -0.0144, -0.0754,  0.1198, -0.0859,\n",
      "        -0.0794,  0.0065, -0.0134, -0.0152,  0.0463,  0.0728, -0.0163,  0.0868,\n",
      "        -0.0950,  0.0857, -0.0169, -0.0176, -0.1219, -0.0547, -0.0197,  0.0904],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0177,  0.0739, -0.0219,  ..., -0.0744,  0.0301, -0.0184],\n",
      "        [-0.0553,  0.0045, -0.0593,  ...,  0.0023, -0.0078,  0.0262],\n",
      "        [ 0.0576,  0.0006, -0.0151,  ..., -0.0199,  0.0334,  0.0152],\n",
      "        ...,\n",
      "        [-0.0314,  0.0259, -0.0453,  ..., -0.0318,  0.0148, -0.0222],\n",
      "        [-0.0573, -0.0110,  0.0833,  ...,  0.0731, -0.0746,  0.0252],\n",
      "        [ 0.0426,  0.0357, -0.0631,  ..., -0.0156, -0.0460,  0.0144]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0373,  0.0086,  0.0841,  0.0012,  0.0316, -0.0269, -0.0776, -0.0882,\n",
      "        -0.0200, -0.0292, -0.0856, -0.0785,  0.0127, -0.0443, -0.0343,  0.0325,\n",
      "        -0.0661,  0.0137,  0.0612,  0.0477,  0.0603, -0.0182, -0.0009, -0.0663,\n",
      "         0.0445, -0.0342,  0.0291,  0.0721, -0.0315,  0.0498, -0.0106,  0.0223,\n",
      "         0.0728, -0.0187, -0.0227, -0.0358, -0.0667,  0.0753, -0.0042, -0.0806,\n",
      "         0.0043,  0.0066,  0.0144, -0.0467,  0.0098,  0.0187,  0.0812,  0.0471,\n",
      "         0.0455,  0.0400,  0.0429,  0.0837, -0.0458, -0.0691, -0.0019, -0.0078,\n",
      "        -0.0363,  0.0612, -0.0663, -0.0438,  0.0041, -0.0020,  0.0559, -0.0179],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1072,  0.0282,  0.0924,  0.0476, -0.0574,  0.0421,  0.0820, -0.0600,\n",
      "          0.0179,  0.0015, -0.1219,  0.0077, -0.0799,  0.1208, -0.0597, -0.0958,\n",
      "          0.0663,  0.0785, -0.0093,  0.0754, -0.0916,  0.1027, -0.0911, -0.0257,\n",
      "         -0.0878, -0.0833,  0.0968,  0.1084,  0.0952, -0.1003,  0.0704, -0.0827,\n",
      "          0.0395, -0.0144,  0.0163, -0.0379, -0.0950,  0.0121, -0.0314, -0.0216,\n",
      "          0.0363, -0.1163,  0.1141, -0.0313,  0.0897, -0.1003,  0.0508, -0.0925,\n",
      "          0.0628, -0.0746, -0.0736, -0.0940, -0.0452,  0.0585,  0.0304,  0.0600,\n",
      "         -0.0632,  0.0348, -0.1023, -0.0410, -0.0339, -0.0650,  0.0472,  0.1006],\n",
      "        [-0.0775, -0.1126, -0.0971, -0.0318, -0.0860, -0.1146, -0.1026,  0.0288,\n",
      "         -0.0467,  0.0529, -0.0820, -0.0642,  0.0859,  0.0769,  0.0653,  0.1112,\n",
      "          0.0090,  0.1205, -0.0948,  0.0071,  0.1147,  0.0566,  0.0240, -0.0206,\n",
      "         -0.0940, -0.1228,  0.0820, -0.1043,  0.0164,  0.1066,  0.1145,  0.0056,\n",
      "         -0.0410, -0.0058,  0.0548,  0.0874,  0.0427, -0.1050,  0.0684, -0.0703,\n",
      "         -0.1233, -0.0359,  0.0781, -0.0714, -0.0562,  0.0327, -0.0375, -0.0665,\n",
      "         -0.0999, -0.0340,  0.0650,  0.1098, -0.1029, -0.1244, -0.0980, -0.0734,\n",
      "         -0.0334, -0.0379, -0.0125,  0.0750, -0.0249, -0.0989, -0.0128, -0.0977]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0220, -0.1152], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1184,  0.0009, -0.0097,  0.0376,  0.0478,  0.0622, -0.0245, -0.0556,\n",
      "         -0.0283, -0.1025, -0.1210,  0.0814, -0.0216, -0.1216,  0.0215, -0.0202,\n",
      "          0.0392, -0.0454, -0.0391, -0.0790,  0.0026, -0.1133, -0.0999, -0.1230,\n",
      "         -0.0219,  0.0030, -0.0296, -0.0733, -0.0347,  0.0215,  0.0597,  0.0611,\n",
      "          0.0948,  0.0793, -0.0588,  0.0668,  0.0715,  0.0921,  0.0638, -0.0389,\n",
      "         -0.1215,  0.0280, -0.0110, -0.0152,  0.0128,  0.1129, -0.0185, -0.1145,\n",
      "          0.1073,  0.0647,  0.0591, -0.0620,  0.0459, -0.0163, -0.0941, -0.1102,\n",
      "         -0.0574, -0.0133, -0.0829, -0.0224,  0.0715,  0.0453,  0.0776,  0.0199]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0805], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in tqs.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://discuss.pytorch.org/t/example-for-one-of-the-differentiated-tensors-appears-to-not-have-been-used-in-the-graph/58396\n",
    "a = torch.rand(10, requires_grad=True)\n",
    "b = torch.rand(10, requires_grad=True)\n",
    "\n",
    "output = (2 * a).sum()\n",
    "\n",
    "torch.autograd.grad(output, (a), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[639], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m((d_ln_P_d_theta \u001b[38;5;241m:=\u001b[39m tqs\u001b[38;5;241m.\u001b[39md_ln_P_dtheta(psi_x)))\n",
      "File \u001b[0;32m~/Projects/new-embedding/model.py:170\u001b[0m, in \u001b[0;36mTQS.d_ln_P_dtheta\u001b[0;34m(self, psi_x)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# psi_x: (seq, batch)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# the problem is that we're not preserving the batch dimension; we end up summing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    162\u001b[0m get_batched_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    163\u001b[0m     torch\u001b[38;5;241m.\u001b[39mlog(psi_x \u001b[38;5;241m*\u001b[39m psi_x\u001b[38;5;241m.\u001b[39mconj()),\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m )\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vmap(get_batched_grad, in_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, randomness\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(torch\u001b[38;5;241m.\u001b[39meye(batch))\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    282\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/Projects/new-embedding/model.py:162\u001b[0m, in \u001b[0;36mTQS.d_ln_P_dtheta.<locals>.<lambda>\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    142\u001b[0m     first_param \u001b[38;5;241m=\u001b[39m param\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# psi_x: (seq, batch)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# the problem is that we're not preserving the batch dimension; we end up summing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m get_batched_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m    163\u001b[0m     torch\u001b[38;5;241m.\u001b[39mlog(psi_x \u001b[38;5;241m*\u001b[39m psi_x\u001b[38;5;241m.\u001b[39mconj()),\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    165\u001b[0m     create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m     grad_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    167\u001b[0m     allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vmap(get_batched_grad, in_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, randomness\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(torch\u001b[38;5;241m.\u001b[39meye(batch))\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _make_grads(\n\u001b[1;32m    386\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:82\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     80\u001b[0m     shape_matches \u001b[38;5;241m=\u001b[39m expect_true(sym_eq(out\u001b[38;5;241m.\u001b[39msize(), first_grad\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_matches:\n\u001b[0;32m---> 82\u001b[0m     out_shape, grad_shape \u001b[38;5;241m=\u001b[39m _calculate_shape(\n\u001b[1;32m     83\u001b[0m         out, first_grad, is_grads_batched\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:45\u001b[0m, in \u001b[0;36m_calculate_shape\u001b[0;34m(output, grad, is_grads_batched)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_shape\u001b[39m(\n\u001b[1;32m     41\u001b[0m     output: torch\u001b[38;5;241m.\u001b[39mTensor, grad: torch\u001b[38;5;241m.\u001b[39mTensor, is_grads_batched: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_ShapeorNestedShape, _ShapeorNestedShape]:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# is_same_size ensures that both tensors are either nested or non nested\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# circular import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NestedTensor\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, NestedTensor):\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:416\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ret_nt, offsets, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_contiguous \u001b[38;5;28;01melse\u001b[39;00m length_list)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# NB: A dummy arg is required so that NestedTensor.__torch_dispatch__() is invoked\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# for _nested_view_from_values_offsets(). Sizes don't matter much, but they shouldn't be\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# 0/1 because the dummy can be fake-ified and we want to avoid specializing.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# This arg is otherwise unused.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m _nt_view_dummy \u001b[38;5;241m=\u001b[39m NestedTensor(\n\u001b[1;32m    417\u001b[0m     values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    418\u001b[0m     offsets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    419\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnested_view_from_values_offsets\u001b[39m(values, offsets, ragged_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_nested_view_from_jagged(\n\u001b[1;32m    424\u001b[0m         values, offsets, _nt_view_dummy, \u001b[38;5;28;01mNone\u001b[39;00m, ragged_idx\n\u001b[1;32m    425\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:112\u001b[0m, in \u001b[0;36mNestedTensor.__init__\u001b[0;34m(self, values, offsets, lengths, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_cache \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# collapsed ragged dim must always be dynamic\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx)\n\u001b[1;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:223\u001b[0m, in \u001b[0;36mmark_dynamic\u001b[0;34m(t, index, min, max)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mMark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_traceable_wrapper_subclass(t):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# TODO: Make this configurable via a supported public API\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     _apply_func_to_inner_tensors_of_same_dim(\n\u001b[1;32m    224\u001b[0m         mark_dynamic, t, index, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dynamo_dynamic_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:178\u001b[0m, in \u001b[0;36m_apply_func_to_inner_tensors_of_same_dim\u001b[0;34m(func, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    177\u001b[0m     inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(t, attr)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inner\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39mdim():\n\u001b[1;32m    179\u001b[0m         func(inner, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:232\u001b[0m, in \u001b[0;36mNestedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:216\u001b[0m, in \u001b[0;36mNestedTensor.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m fn \u001b[38;5;241m=\u001b[39m lookup_jagged(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(func)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:181\u001b[0m, in \u001b[0;36mregister_func.<locals>.wrapper.<locals>.get_inner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 181\u001b[0m     check_schema(schema_str, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(aten_op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:118\u001b[0m, in \u001b[0;36mcheck_schema\u001b[0;34m(schema_str, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_fn(args[i]):\n\u001b[1;32m    110\u001b[0m     type_to_desc \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt?\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptional tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<any type>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     }\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNestedTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_to_desc[arg_type]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor"
     ]
    }
   ],
   "source": [
    "print((d_ln_P_d_theta := tqs.d_ln_P_dtheta(psi_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(d_ln_P_d_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([192])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([192, 64])\n",
      "torch.Size([192])\n",
      "torch.Size([64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64])\n",
      "torch.Size([128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([2, 64])\n",
      "torch.Size([2])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters shapes of the model\n",
    "for i in tqs.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.9367,  0.2668, -1.6303, -1.7862,  0.8277,  0.1077,  1.8067, -2.5265,\n",
      "         0.7316, -1.4697, -0.4042, -0.8399, -0.2293,  0.3884,  0.7997,  1.0304,\n",
      "         0.6088, -0.0918, -0.6525, -0.7985,  0.5418,  0.5808,  0.5402,  0.6873,\n",
      "         0.1134,  0.5988, -1.9142, -0.8318,  0.6114, -0.2724, -0.2606, -1.3139,\n",
      "         1.0847,  1.9056, -1.2572,  1.5666,  0.2288, -0.4180,  0.6461, -0.3176,\n",
      "        -0.3120, -0.2958, -0.3488, -0.3325, -0.6533, -0.8155, -0.2047,  0.2941,\n",
      "        -1.8817, -2.4597, -1.3084,  1.9289,  1.8321, -0.0902,  0.4203,  1.6066,\n",
      "        -0.7052, -0.0258, -0.1230,  0.3112,  1.2757,  0.0777, -1.9691, -0.9766],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4751, -0.3406,  0.2423,  0.2803,  1.7058,  0.7552, -0.9422,  0.0115,\n",
      "         0.9143,  0.5812,  0.8512, -0.0914,  0.6948, -0.4445,  0.3403, -1.9351,\n",
      "        -0.4757, -0.5741, -1.0688,  0.9787, -0.3464, -0.4479,  1.8879, -0.2108,\n",
      "         1.2273,  0.2867,  0.2962, -1.8591,  1.5110,  0.1349, -0.5054,  1.1011,\n",
      "        -1.2408, -0.1704,  0.2568,  2.4227, -0.5757,  0.9428,  0.1160, -0.5598,\n",
      "         1.3906, -1.3730, -1.4973,  2.1498,  1.0838,  1.2060,  0.0827, -0.4469,\n",
      "        -1.2853, -0.4454, -1.0298, -0.3567,  1.2075,  0.2536,  0.3128,  1.4835,\n",
      "         0.6587, -0.1873,  0.0933,  0.9059, -0.5005,  1.1766, -1.0223, -0.6396],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1149, -0.1347, -0.0496,  ..., -0.1256,  0.1527,  0.1075],\n",
      "        [ 0.0944, -0.0440, -0.0170,  ..., -0.1480, -0.1399,  0.0873],\n",
      "        [ 0.1318, -0.0547, -0.1052,  ..., -0.0511, -0.0090, -0.0238],\n",
      "        ...,\n",
      "        [ 0.1293,  0.0152, -0.0326,  ..., -0.1025, -0.0556, -0.0271],\n",
      "        [ 0.1436, -0.1439, -0.1245,  ...,  0.0227, -0.0246,  0.0381],\n",
      "        [ 0.0909,  0.0142,  0.1043,  ..., -0.0937, -0.0422, -0.0643]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1062,  0.0109, -0.1124,  ..., -0.0729, -0.0937,  0.0368],\n",
      "        [ 0.1176,  0.0314, -0.1170,  ...,  0.0160, -0.0010, -0.0082],\n",
      "        [ 0.0211, -0.0214,  0.0657,  ...,  0.1021,  0.0578, -0.0588],\n",
      "        ...,\n",
      "        [-0.0508,  0.0651,  0.0006,  ...,  0.0862,  0.0667, -0.0229],\n",
      "        [-0.0698, -0.0941,  0.0464,  ..., -0.0796, -0.1065,  0.0467],\n",
      "        [ 0.0699, -0.0922,  0.1235,  ...,  0.0882,  0.0830, -0.0017]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0306, -0.0249, -0.1003,  ..., -0.0059,  0.0440,  0.0800],\n",
      "        [ 0.0211, -0.1176,  0.0246,  ..., -0.1231, -0.0766,  0.0786],\n",
      "        [-0.0206, -0.0829,  0.1160,  ...,  0.0108,  0.1110, -0.0402],\n",
      "        ...,\n",
      "        [ 0.1221,  0.0444,  0.0392,  ..., -0.0126,  0.0745, -0.1012],\n",
      "        [ 0.0357,  0.1098, -0.0641,  ...,  0.1066,  0.1060, -0.0197],\n",
      "        [-0.0472,  0.0338,  0.0647,  ...,  0.0738, -0.0995,  0.0832]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0008,  0.0911, -0.0113,  0.1153,  0.1171,  0.0311, -0.0924,  0.0237,\n",
      "         0.0023, -0.0386,  0.0856, -0.0203, -0.0233, -0.1138, -0.0206, -0.0763,\n",
      "         0.0215, -0.0423,  0.0174,  0.0516, -0.0791, -0.0496,  0.0126,  0.0663,\n",
      "        -0.0018, -0.0627,  0.0235, -0.0009,  0.0128, -0.0129,  0.0020,  0.0974,\n",
      "         0.1179,  0.0754, -0.0162, -0.0137, -0.0701, -0.0924,  0.0999,  0.0522,\n",
      "        -0.0044, -0.0404,  0.0304,  0.0730,  0.1178,  0.0276,  0.0704, -0.0837,\n",
      "        -0.0858, -0.1130, -0.0430, -0.0718, -0.0678,  0.0016, -0.0384, -0.0156,\n",
      "         0.0390, -0.0676,  0.1237,  0.1005,  0.0481,  0.0152, -0.0381,  0.0277,\n",
      "         0.0509,  0.0216, -0.1022,  0.0772, -0.0361, -0.1023, -0.0942, -0.0157,\n",
      "        -0.0652,  0.0941, -0.0559, -0.0268,  0.0633, -0.0403,  0.0709,  0.0996,\n",
      "         0.0144, -0.0366,  0.0572, -0.0521, -0.0048, -0.0786,  0.0161, -0.0230,\n",
      "         0.0797, -0.0851, -0.0757,  0.1200,  0.0874,  0.1189,  0.0557, -0.0290,\n",
      "        -0.0927,  0.0307, -0.0427,  0.0793, -0.0448, -0.0563,  0.0929, -0.1033,\n",
      "         0.0317, -0.0581, -0.0173,  0.0718,  0.0926,  0.0088,  0.1004,  0.0304,\n",
      "        -0.0071,  0.0549,  0.0532,  0.0830, -0.0180,  0.0772, -0.0814, -0.0578,\n",
      "         0.0324,  0.1022, -0.1222, -0.1185, -0.1069,  0.0104,  0.0588, -0.0220],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0207,  0.0600,  0.0130,  ..., -0.0748, -0.0222, -0.0631],\n",
      "        [ 0.0419,  0.0116,  0.0649,  ..., -0.0311,  0.0588,  0.0235],\n",
      "        [-0.0103, -0.0396,  0.0598,  ...,  0.0085,  0.0425,  0.0467],\n",
      "        ...,\n",
      "        [-0.0869, -0.0407, -0.0674,  ...,  0.0753,  0.0837,  0.0140],\n",
      "        [ 0.0252,  0.0235, -0.0638,  ..., -0.0686,  0.0269, -0.0069],\n",
      "        [-0.0847,  0.0605,  0.0573,  ...,  0.0326, -0.0629, -0.0440]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0776,  0.0346,  0.0793,  0.0731,  0.0696,  0.0030, -0.0377, -0.0556,\n",
      "         0.0344, -0.0548,  0.0184, -0.0321,  0.0742, -0.0556,  0.0409,  0.0419,\n",
      "        -0.0238,  0.0198,  0.0033,  0.0793,  0.0832, -0.0613, -0.0249,  0.0132,\n",
      "         0.0736, -0.0222,  0.0871, -0.0441,  0.0734, -0.0464,  0.0500,  0.0376,\n",
      "        -0.0742,  0.0844,  0.0290,  0.0692, -0.0649, -0.0116,  0.0810, -0.0610,\n",
      "        -0.0787,  0.0497, -0.0273,  0.0241, -0.0079, -0.0389,  0.0428,  0.0281,\n",
      "        -0.0169,  0.0247, -0.0392,  0.0708,  0.0741,  0.0104,  0.0397, -0.0313,\n",
      "         0.0338,  0.0514, -0.0297,  0.0704,  0.0108, -0.0238, -0.0828,  0.0850],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1149, -0.1347, -0.0496,  ..., -0.1256,  0.1527,  0.1075],\n",
      "        [ 0.0944, -0.0440, -0.0170,  ..., -0.1480, -0.1399,  0.0873],\n",
      "        [ 0.1318, -0.0547, -0.1052,  ..., -0.0511, -0.0090, -0.0238],\n",
      "        ...,\n",
      "        [ 0.1293,  0.0152, -0.0326,  ..., -0.1025, -0.0556, -0.0271],\n",
      "        [ 0.1436, -0.1439, -0.1245,  ...,  0.0227, -0.0246,  0.0381],\n",
      "        [ 0.0909,  0.0142,  0.1043,  ..., -0.0937, -0.0422, -0.0643]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1062,  0.0109, -0.1124,  ..., -0.0729, -0.0937,  0.0368],\n",
      "        [ 0.1176,  0.0314, -0.1170,  ...,  0.0160, -0.0010, -0.0082],\n",
      "        [ 0.0211, -0.0214,  0.0657,  ...,  0.1021,  0.0578, -0.0588],\n",
      "        ...,\n",
      "        [-0.0508,  0.0651,  0.0006,  ...,  0.0862,  0.0667, -0.0229],\n",
      "        [-0.0698, -0.0941,  0.0464,  ..., -0.0796, -0.1065,  0.0467],\n",
      "        [ 0.0699, -0.0922,  0.1235,  ...,  0.0882,  0.0830, -0.0017]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0306, -0.0249, -0.1003,  ..., -0.0059,  0.0440,  0.0800],\n",
      "        [ 0.0211, -0.1176,  0.0246,  ..., -0.1231, -0.0766,  0.0786],\n",
      "        [-0.0206, -0.0829,  0.1160,  ...,  0.0108,  0.1110, -0.0402],\n",
      "        ...,\n",
      "        [ 0.1221,  0.0444,  0.0392,  ..., -0.0126,  0.0745, -0.1012],\n",
      "        [ 0.0357,  0.1098, -0.0641,  ...,  0.1066,  0.1060, -0.0197],\n",
      "        [-0.0472,  0.0338,  0.0647,  ...,  0.0738, -0.0995,  0.0832]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0008,  0.0911, -0.0113,  0.1153,  0.1171,  0.0311, -0.0924,  0.0237,\n",
      "         0.0023, -0.0386,  0.0856, -0.0203, -0.0233, -0.1138, -0.0206, -0.0763,\n",
      "         0.0215, -0.0423,  0.0174,  0.0516, -0.0791, -0.0496,  0.0126,  0.0663,\n",
      "        -0.0018, -0.0627,  0.0235, -0.0009,  0.0128, -0.0129,  0.0020,  0.0974,\n",
      "         0.1179,  0.0754, -0.0162, -0.0137, -0.0701, -0.0924,  0.0999,  0.0522,\n",
      "        -0.0044, -0.0404,  0.0304,  0.0730,  0.1178,  0.0276,  0.0704, -0.0837,\n",
      "        -0.0858, -0.1130, -0.0430, -0.0718, -0.0678,  0.0016, -0.0384, -0.0156,\n",
      "         0.0390, -0.0676,  0.1237,  0.1005,  0.0481,  0.0152, -0.0381,  0.0277,\n",
      "         0.0509,  0.0216, -0.1022,  0.0772, -0.0361, -0.1023, -0.0942, -0.0157,\n",
      "        -0.0652,  0.0941, -0.0559, -0.0268,  0.0633, -0.0403,  0.0709,  0.0996,\n",
      "         0.0144, -0.0366,  0.0572, -0.0521, -0.0048, -0.0786,  0.0161, -0.0230,\n",
      "         0.0797, -0.0851, -0.0757,  0.1200,  0.0874,  0.1189,  0.0557, -0.0290,\n",
      "        -0.0927,  0.0307, -0.0427,  0.0793, -0.0448, -0.0563,  0.0929, -0.1033,\n",
      "         0.0317, -0.0581, -0.0173,  0.0718,  0.0926,  0.0088,  0.1004,  0.0304,\n",
      "        -0.0071,  0.0549,  0.0532,  0.0830, -0.0180,  0.0772, -0.0814, -0.0578,\n",
      "         0.0324,  0.1022, -0.1222, -0.1185, -0.1069,  0.0104,  0.0588, -0.0220],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0207,  0.0600,  0.0130,  ..., -0.0748, -0.0222, -0.0631],\n",
      "        [ 0.0419,  0.0116,  0.0649,  ..., -0.0311,  0.0588,  0.0235],\n",
      "        [-0.0103, -0.0396,  0.0598,  ...,  0.0085,  0.0425,  0.0467],\n",
      "        ...,\n",
      "        [-0.0869, -0.0407, -0.0674,  ...,  0.0753,  0.0837,  0.0140],\n",
      "        [ 0.0252,  0.0235, -0.0638,  ..., -0.0686,  0.0269, -0.0069],\n",
      "        [-0.0847,  0.0605,  0.0573,  ...,  0.0326, -0.0629, -0.0440]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0776,  0.0346,  0.0793,  0.0731,  0.0696,  0.0030, -0.0377, -0.0556,\n",
      "         0.0344, -0.0548,  0.0184, -0.0321,  0.0742, -0.0556,  0.0409,  0.0419,\n",
      "        -0.0238,  0.0198,  0.0033,  0.0793,  0.0832, -0.0613, -0.0249,  0.0132,\n",
      "         0.0736, -0.0222,  0.0871, -0.0441,  0.0734, -0.0464,  0.0500,  0.0376,\n",
      "        -0.0742,  0.0844,  0.0290,  0.0692, -0.0649, -0.0116,  0.0810, -0.0610,\n",
      "        -0.0787,  0.0497, -0.0273,  0.0241, -0.0079, -0.0389,  0.0428,  0.0281,\n",
      "        -0.0169,  0.0247, -0.0392,  0.0708,  0.0741,  0.0104,  0.0397, -0.0313,\n",
      "         0.0338,  0.0514, -0.0297,  0.0704,  0.0108, -0.0238, -0.0828,  0.0850],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0340, -0.0425, -0.0054,  0.1040,  0.1203,  0.0581,  0.0395, -0.0806,\n",
      "         -0.0247,  0.0101,  0.0968,  0.0908, -0.0726,  0.0258,  0.0658, -0.0174,\n",
      "          0.0691,  0.0265, -0.0410, -0.0930,  0.0755,  0.0145,  0.0006, -0.0982,\n",
      "          0.0692, -0.0786,  0.0928, -0.0601,  0.0976, -0.0815, -0.0071,  0.0812,\n",
      "         -0.0508, -0.0516, -0.1035, -0.0236, -0.0313,  0.0918,  0.0088,  0.0004,\n",
      "         -0.0241,  0.0384, -0.0074,  0.1222, -0.0451,  0.0739, -0.0247,  0.0151,\n",
      "          0.0092, -0.0535, -0.1100, -0.0001, -0.0967,  0.0237, -0.0129,  0.1105,\n",
      "          0.0948,  0.0362, -0.1009,  0.0856, -0.1028,  0.1130,  0.0572,  0.0427],\n",
      "        [-0.0677,  0.0234,  0.0820, -0.1117,  0.0244, -0.0408,  0.0728,  0.0321,\n",
      "         -0.1027, -0.0101,  0.0037,  0.0585, -0.0635,  0.0341, -0.0280, -0.0402,\n",
      "         -0.0872,  0.0893,  0.0306, -0.0247,  0.1050,  0.1040,  0.0165, -0.1001,\n",
      "         -0.1029, -0.0873,  0.1131, -0.0796,  0.0870,  0.0185,  0.1190,  0.0269,\n",
      "         -0.0368,  0.0401, -0.0441,  0.0808, -0.0197,  0.0168, -0.0033,  0.1197,\n",
      "          0.1091, -0.0640, -0.0926,  0.0246, -0.0253,  0.0938,  0.0569,  0.0197,\n",
      "         -0.1231, -0.0273,  0.0928, -0.0922, -0.0781, -0.0786, -0.0679,  0.0266,\n",
      "          0.0848,  0.0243, -0.1164, -0.0722, -0.0102,  0.0094, -0.0417,  0.0714]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0342, -0.0953], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0223, -0.0473, -0.0334, -0.0805,  0.0329, -0.1023, -0.1101, -0.1047,\n",
      "         -0.0447, -0.0402,  0.0592, -0.0387,  0.0175,  0.0406, -0.0143, -0.1007,\n",
      "         -0.0161,  0.0081, -0.0155,  0.0272, -0.0520, -0.0450,  0.0124,  0.0935,\n",
      "          0.0562, -0.0714,  0.1098, -0.0240, -0.0999, -0.0235, -0.0668, -0.0268,\n",
      "         -0.0995,  0.0922, -0.0052, -0.0761,  0.0055,  0.1096,  0.1188,  0.0370,\n",
      "          0.0851, -0.0887, -0.0237, -0.0856,  0.0426, -0.0323,  0.0952,  0.0578,\n",
      "          0.1157,  0.0449,  0.0859, -0.1074,  0.0941, -0.0519, -0.1156,  0.0317,\n",
      "          0.0389,  0.1086,  0.1094,  0.1241,  0.0286,  0.0927, -0.0611,  0.0182]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1078], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in tqs.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([192, 64]) torch.Size([192, 64])\n",
      "torch.Size([192]) torch.Size([192])\n",
      "torch.Size([64, 64]) torch.Size([64, 64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([128, 64]) torch.Size([128, 64])\n",
      "torch.Size([128]) torch.Size([128])\n",
      "torch.Size([64, 128]) torch.Size([64, 128])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([64]) torch.Size([64])\n",
      "torch.Size([2, 64]) torch.Size([2, 64])\n",
      "torch.Size([2]) torch.Size([2])\n",
      "torch.Size([1, 64]) torch.Size([1, 64])\n",
      "torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for grad, param in zip(d_ln_P_d_theta, tqs.parameters()):\n",
    "    if grad is not None:\n",
    "        print(grad.shape, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 12.0460,  -5.3368,  -5.4714,   6.7329, -21.8639,  16.2115,  -1.8841,\n",
       "           2.7430,   6.1641,  25.9044,  -1.0642,   8.0647, -14.6524,   1.7242,\n",
       "           3.3404,  19.5245,  -1.9994,  16.3765,   3.0707,  -3.7363,   5.9793,\n",
       "          -3.7739,  16.6138,  -8.9862,   9.6171, -33.6170,  14.7179,  -7.1137,\n",
       "          10.2698,   0.3457,   9.2687,   1.9637,  -3.1346, -19.9616,   6.5355,\n",
       "           5.2249,  11.2312,   8.6126,  -4.7546,   2.2451,   3.1444, -15.6841,\n",
       "          -0.5689,  -1.4260,  23.9463,   7.9263,   2.7100,   7.9186,  -0.6613,\n",
       "           6.2767,  -8.4629,  -2.8857,  -9.8538,   7.5645, -11.8798,  13.5053,\n",
       "          -9.0124,  16.5284,  -0.6507,  14.4156, -19.2918,  -6.8954,  -2.0077,\n",
       "           8.1826], grad_fn=<ViewBackward0>),\n",
       " tensor([-1.4482,  0.8351,  1.4949, -1.6435,  0.7335, -1.1153,  0.9714,  0.2423,\n",
       "         -2.1453, -1.2152,  0.3682,  0.1144, -0.5094,  0.1646, -0.4671, -0.9084,\n",
       "         -1.0607,  0.3759,  0.3585,  0.0600,  0.9356,  1.6283, -0.6500, -0.7605,\n",
       "         -1.3930,  0.1979,  0.8508, -0.2695, -0.0847,  0.0198,  1.3698, -0.1712,\n",
       "         -0.4297,  1.7634, -0.4233,  0.4039, -0.6001,  0.1792,  0.2397,  1.2434,\n",
       "          1.2910, -0.7106, -0.9060, -0.2231, -1.1202,  0.5364,  0.1718,  0.5383,\n",
       "         -1.6035, -0.4522,  1.5999, -1.3570, -0.3505, -1.1142, -0.7784, -0.6054,\n",
       "          1.0357, -0.2342, -1.3254, -1.8747,  0.2195, -0.0214, -0.8010,  0.6199],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " tensor([[-1.0758e+00, -3.2617e-01,  9.3843e-01,  ..., -2.9942e+01,\n",
       "           1.5681e-01, -2.9484e+01],\n",
       "         [ 1.1306e-01,  7.4768e-03, -7.0891e-02,  ...,  3.2736e+00,\n",
       "          -2.0165e-02,  3.2188e+00],\n",
       "         [-5.7658e-02,  1.1248e-01,  1.3937e-02,  ..., -1.7406e+00,\n",
       "           2.0713e-02, -1.6952e+00],\n",
       "         ...,\n",
       "         [-1.4849e+01, -6.0784e+00,  3.1440e+01,  ...,  9.0570e+01,\n",
       "           3.6458e+01,  1.0877e+02],\n",
       "         [-8.8040e+00, -3.6471e+00,  1.8761e+01,  ...,  5.7345e+01,\n",
       "           2.1715e+01,  6.8139e+01],\n",
       "         [ 7.2676e+00,  3.0779e+00, -1.5688e+01,  ..., -4.9337e+01,\n",
       "          -1.8128e+01, -5.8318e+01]], grad_fn=<TBackward0>),\n",
       " tensor([-2.9646e+01,  3.2381e+00, -1.7112e+00,  1.5958e+01,  1.9689e+01,\n",
       "         -4.3722e+01, -3.1969e+01,  2.3443e+01,  2.5034e+01, -1.5603e+01,\n",
       "         -1.4305e+01,  2.5567e+01, -2.5172e+00,  1.6268e+01,  5.1950e+00,\n",
       "         -3.6194e+01, -2.4798e+00, -2.6066e+01, -6.8854e+00,  3.1431e+01,\n",
       "          3.7500e+01, -9.5716e+00,  6.5639e+00,  9.9447e+00,  1.7616e+00,\n",
       "          2.7414e+01,  3.7529e+01,  1.3813e+01,  5.5600e+00,  1.4701e+01,\n",
       "         -4.0678e+01,  1.8636e+01,  9.7607e+00, -2.3572e+01, -8.9726e+00,\n",
       "         -2.5326e+01, -2.2017e+01,  1.8576e+01,  3.1490e+00, -3.6929e+01,\n",
       "          1.2913e+01, -1.0100e+01,  1.3343e+01,  7.8212e-01, -5.6526e+01,\n",
       "         -1.6500e+01, -6.2152e+00, -2.0195e+01, -1.7589e+01, -3.1791e+00,\n",
       "         -4.0567e+01,  5.2217e+01, -2.4828e+01,  1.4646e+01,  5.8255e+01,\n",
       "         -1.4706e+01,  9.4856e+00, -2.4924e+01, -2.9111e+01, -9.5584e+00,\n",
       "         -3.9023e+01,  3.7101e+01, -1.8337e+00, -2.5471e+01,  6.8918e-08,\n",
       "         -6.7055e-08,  1.0058e-07,  1.6391e-07, -8.3819e-08,  3.3528e-08,\n",
       "         -2.9802e-08, -1.0431e-07,  0.0000e+00,  1.6764e-08,  2.0862e-07,\n",
       "          8.1956e-08, -2.7940e-09,  8.1956e-08,  6.3330e-08, -2.7940e-08,\n",
       "         -7.8697e-08, -7.4506e-08, -3.3528e-08,  7.1712e-08,  3.2037e-07,\n",
       "         -3.7253e-08, -2.0117e-07,  3.6508e-07,  1.4901e-07, -1.5646e-07,\n",
       "          2.6077e-08,  5.8115e-07,  2.0489e-08,  1.7136e-07,  3.7253e-08,\n",
       "         -4.8429e-08, -4.2841e-08, -3.7253e-08,  2.9802e-08, -1.0803e-07,\n",
       "         -4.4703e-08,  3.7253e-08, -6.7055e-08,  0.0000e+00, -7.4506e-08,\n",
       "          5.2154e-08,  2.0489e-08,  3.3528e-08, -8.5682e-08,  5.9605e-07,\n",
       "          1.6391e-07,  0.0000e+00,  1.1921e-07, -1.7695e-08, -5.9605e-08,\n",
       "          2.6077e-07,  3.9116e-08,  8.1956e-08, -5.2154e-08, -2.0862e-07,\n",
       "          1.5646e-07,  4.2841e-08, -4.0978e-08, -2.9802e-08,  1.4901e-07,\n",
       "          1.9744e-07,  2.9802e-08,  1.3411e-07,  1.5741e+01,  8.4038e+01,\n",
       "         -7.8192e+01,  7.1320e+01, -1.0958e+02,  1.0698e+02, -6.0031e+01,\n",
       "         -3.0038e+00,  3.7204e+00, -6.2448e+01, -2.3399e+01,  3.9813e+01,\n",
       "          4.2283e+01, -4.7650e+01,  3.3535e+01,  5.8268e+00,  4.6890e+01,\n",
       "         -1.7110e+02,  1.7624e+00,  6.2257e+01, -6.8823e+00, -4.7385e+01,\n",
       "          6.4677e+01, -5.0766e+01, -2.8569e+01, -9.5344e+01, -3.5944e+01,\n",
       "         -3.0294e+00,  1.2633e+01,  8.3078e+00,  1.4016e+01,  8.6170e+01,\n",
       "         -1.7685e+01, -3.8470e+01,  2.5520e+01, -2.6657e+01, -1.2764e+01,\n",
       "         -4.8096e+01,  7.6159e-01, -6.7124e+01, -2.2995e+01, -2.2212e+01,\n",
       "          3.7169e+01, -1.3690e+01,  7.6815e+00,  1.9460e+01,  6.7804e+00,\n",
       "         -7.3712e+01, -6.1495e+00,  2.9846e+01,  1.6483e+01, -9.7684e+00,\n",
       "         -8.4677e+01, -8.1769e+01,  5.8071e+01, -5.1772e+01,  4.0807e+01,\n",
       "         -2.8450e+01,  1.3667e+01, -2.7248e+01, -6.1022e+01,  9.0971e+01,\n",
       "          5.7545e+01, -4.9480e+01], grad_fn=<ViewBackward0>),\n",
       " tensor([[  0.7543,  41.4062, -33.3401,  ...,   2.9278,  14.7004, -35.8029],\n",
       "         [  3.2872, -17.4195,  20.1845,  ...,   1.0794,  -5.4357,  15.9621],\n",
       "         [  0.2644, -57.4890,  48.5597,  ...,  -3.2457, -20.1536,  50.0487],\n",
       "         ...,\n",
       "         [ -0.4775,  -1.6958,   0.6098,  ...,  -0.3881,  -0.6594,   1.3547],\n",
       "         [ -1.0715,  36.7660, -32.5803,  ...,   1.4847,  12.7257, -32.2213],\n",
       "         [ -0.3656, -60.5149,  49.9895,  ...,  -3.8061, -21.3686,  52.5022]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([ -71.5833,   36.5131,  101.7186, -137.0842,   20.2367,  -30.1991,\n",
       "           80.7271,   41.2087, -147.1909,   -5.8409,   29.8020,   65.2213,\n",
       "          -50.8602,   22.8549,  -17.7327,  -46.2873,  -79.5682,   92.7451,\n",
       "           50.2928,  -21.2522,  124.6082,  125.4182,   -3.4944, -125.0116,\n",
       "         -134.1794, -128.3991,  133.0302,  -73.9804,   98.8873,   15.6790,\n",
       "          154.4969,   27.5897,  -64.9665,   58.8066,  -50.2947,  105.6436,\n",
       "          -31.3541,   43.4165,   -4.0200,  182.4493,  115.6465,  -85.5516,\n",
       "         -121.1594,   13.1944,  -50.4834,  105.4286,   93.8919,   28.9109,\n",
       "         -167.7226,  -26.7217,  139.7401, -125.3938,  -77.0902, -108.9787,\n",
       "          -89.6940,   28.9960,  129.1182,   49.9690, -148.5907, -103.3577,\n",
       "          -29.6247,    2.1512,  -66.6310,  105.9062], grad_fn=<ViewBackward0>),\n",
       " tensor([[ 14.1009,  23.5426,  12.7609,  ..., -16.3514,  11.8230, -24.8607],\n",
       "         [ 24.5244,  42.8022,  17.5638,  ..., -27.2272,  20.0145, -39.9431],\n",
       "         [ 16.7714,  28.6231,  12.7476,  ..., -20.4391,  15.3355, -29.4084],\n",
       "         ...,\n",
       "         [  0.1455,   1.3759,   0.3461,  ...,  -0.9039,   0.8036,  -1.1609],\n",
       "         [  2.5182,   4.6887,   3.3722,  ...,  -3.7850,   2.8445,  -5.1906],\n",
       "         [-11.2469, -14.1559,  -6.5855,  ...,  10.8203,  -7.8174,  17.3501]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([-20.5468, -34.1049, -25.7576,  -3.0904,  -1.2963,   2.6892,  -0.0928,\n",
       "          -8.1693,  23.0133, -31.8499,   0.0000,  -8.5333,   6.9477,   0.0000,\n",
       "          62.1644,   2.4966,  -2.6567,  -0.7705,   0.5204,  10.3993,  -1.5965,\n",
       "           0.6094,   0.0000,   0.5364,  -5.1855,   7.5987,  13.3557, -10.1663,\n",
       "           0.3133,  15.0105,  -3.6924,   4.7107,  22.5416, -11.8616, -11.5486,\n",
       "           0.1603,   2.8340,   2.5026,  -0.9501,   0.7625, -24.4779,  -0.8888,\n",
       "          18.8728, -44.2703,   2.4760,  -1.6480,  -1.1347, -13.9856, -12.9660,\n",
       "         -23.8267,  -2.7003,   3.7520,  -9.9994,   6.4425,   0.0656,   0.0000,\n",
       "          -8.0126,   0.3047, -14.8097, -38.8014,  -2.0307,  41.0631,   0.6843,\n",
       "         -16.2779,   2.0267,   1.9488,   1.9704,  -1.4131,   0.8928,  -6.4718,\n",
       "           5.0564,  -0.4639, -16.3344,   0.7584,  -3.6426,   2.9900,  17.9703,\n",
       "          -6.0191,   2.2837,   1.6199,  -2.2729,  32.6460,  -2.2367,  49.0439,\n",
       "          -0.1463,   0.0000,  -7.6724, -12.6036,   4.1669,   3.4699,   8.4972,\n",
       "          13.4856,   1.7222, -10.2489, -17.4309,  -2.1514,   0.0000, -35.3052,\n",
       "         -11.4870,   8.0258,  -8.0351,   1.7834,  -8.0925, -10.7298,   1.4476,\n",
       "          11.6297,  -1.3037,  -1.3422,  -0.9387, -35.1420,  10.4566,   0.2983,\n",
       "          -1.3407, -20.6899,  -4.3405,  12.8082,  25.7544,   7.8190,   1.4159,\n",
       "           0.8801,   8.5338,   9.4390,   1.4607,  17.5219,  -5.1740,  -1.1993,\n",
       "          -4.8087,  13.5590], grad_fn=<ViewBackward0>),\n",
       " tensor([[-27.6215, -21.1469, -39.5241,  ...,  -5.8436, -17.4518, -12.8965],\n",
       "         [ 10.0101,   7.1181,  14.3438,  ...,   1.9086,   6.1195,   4.8164],\n",
       "         [ 34.1767,  25.6477,  48.9367,  ...,   7.1494,  21.4964,  15.7157],\n",
       "         ...,\n",
       "         [  3.9620,   3.1727,   5.6136,  ...,   0.8990,   2.4821,   1.8415],\n",
       "         [-17.4784, -13.3993, -24.8015,  ...,  -3.6766, -10.8951,  -8.0058],\n",
       "         [ 29.9768,  23.1130,  42.4060,  ...,   6.3943,  18.7688,  13.7403]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([-52.7694,  18.1431,  65.0214, -87.5992,  18.1664, -32.0867,  57.0518,\n",
       "          25.7244, -80.4705,  -7.6006,   3.5638,  46.6503, -48.8280,  28.1482,\n",
       "         -21.3147, -30.0304, -70.4492,  73.3920,  24.0141, -18.9607,  84.0051,\n",
       "          83.7464,  13.8897, -80.0567, -82.7275, -70.2126,  91.0241, -63.2668,\n",
       "          69.9357,  16.1860,  95.7293,  22.7179, -28.8307,  33.0253, -35.0222,\n",
       "          64.8232, -15.4687,  13.3045,  -2.4490,  96.3805,  87.2227, -50.5666,\n",
       "         -74.1924,  19.7239, -19.7600,  76.2312,  45.8686,  15.9389, -98.5955,\n",
       "         -21.2567,  74.1297, -73.7485, -62.9276, -62.6303, -54.1055,  21.6349,\n",
       "          67.4079,  20.2627, -93.2062, -57.6196,  -7.9063,   7.6901, -33.4344,\n",
       "          57.3395], grad_fn=<ViewBackward0>),\n",
       " tensor([ 2.9302e+01, -2.3300e+01, -3.5283e+01,  3.3825e+01, -6.3794e+00,\n",
       "          1.7427e+01, -5.7620e+01, -3.4017e+00, -6.4188e+00,  2.0513e+00,\n",
       "         -1.4392e+01, -1.9643e+01, -3.9297e+00, -2.7939e+00, -1.3175e-01,\n",
       "          3.3120e+01,  2.4156e+01, -5.3528e+01, -6.4985e+00,  8.0447e+00,\n",
       "         -7.0518e+00, -5.2876e+00, -2.5258e+00,  1.4013e+01, -2.9422e+01,\n",
       "         -2.0285e+01,  1.0956e+01, -5.4116e+01, -2.4497e+01,  2.0601e+01,\n",
       "         -5.4660e+01,  1.2261e+01,  1.0126e+01,  2.6024e+01,  2.8429e+01,\n",
       "          6.0350e+01,  9.0710e+00,  2.1357e+01,  2.4506e-01,  9.1503e+01,\n",
       "         -6.5345e+01, -7.6962e+01,  3.0259e+01,  8.4130e+00, -5.4012e-02,\n",
       "          8.8587e+01,  1.0333e+00,  1.5436e+01,  2.0012e+01, -5.6784e+00,\n",
       "         -6.8005e+01, -9.3926e+01,  2.5555e+01, -6.3712e+01,  7.6108e+01,\n",
       "          1.5547e+01, -4.0236e+01,  4.2749e+01,  2.5520e+01, -5.9202e+01,\n",
       "          1.3539e+01,  8.2692e-01,  2.6193e+01,  7.8064e+01],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([ -47.7105,   22.9784,   66.2096,  -90.4253,   12.4875,  -20.5976,\n",
       "           52.0779,   26.4745,  -96.8415,   -4.3713,   18.6812,   41.8411,\n",
       "          -34.0225,   14.7615,  -12.1912,  -30.8996,  -52.8657,   60.1514,\n",
       "           32.3343,  -14.4993,   81.0278,   81.7977,   -2.6073,  -82.4688,\n",
       "          -88.1256,  -84.3879,   86.5880,  -48.5068,   64.1071,   10.6544,\n",
       "          100.6583,   17.6668,  -42.5850,   38.2379,  -33.6024,   68.7694,\n",
       "          -20.8668,   28.0098,   -3.1046,  118.9984,   75.2208,  -55.9618,\n",
       "          -79.8500,    8.6248,  -33.3845,   69.1526,   60.9940,   18.9473,\n",
       "         -110.0472,  -18.1667,   90.5333,  -82.1100,  -51.1567,  -71.5138,\n",
       "          -59.6948,   18.7371,   83.5151,   32.4241,  -97.5779,  -67.8987,\n",
       "          -19.7375,    1.1278,  -43.9545,   69.0615],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([ 24.2402, -23.7201, -20.9406,  14.8465, -16.1388,  25.3187, -56.9687,\n",
       "         -10.7087,  18.3097,   5.9133,  -1.6420, -31.6240,  -8.3245,  -4.6145,\n",
       "           0.9442,  38.0978,  38.5672, -59.5595,  -6.2894,  13.2880, -17.8530,\n",
       "         -15.5165,   8.0719,   1.6639, -23.4110,  -8.6710,  23.5884, -71.1189,\n",
       "         -14.2357,  28.1160, -66.2578,  12.0874,   6.1172,  15.7566,  23.8483,\n",
       "          62.7114,   9.1825,   5.6171,   0.6376,  71.2127, -84.9762, -83.9447,\n",
       "          56.9450,  17.8212,  -4.6260, 109.0161,   0.8780,  14.9122,  23.8354,\n",
       "          -4.2893, -67.5613, -82.7566,  21.9419, -47.2480,  41.6706,  23.3133,\n",
       "         -36.9860,  27.4235,  27.0489, -65.4897,   5.9562,   4.3590,  24.2989,\n",
       "          74.9359], grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([ -55.9855,   19.3038,   67.7410,  -92.2772,   20.1760,  -33.7197,\n",
       "           60.1803,   26.5373,  -84.8759,   -8.3334,    3.0507,   48.3235,\n",
       "          -52.4504,   28.1519,  -23.1286,  -33.2306,  -72.0769,   73.7685,\n",
       "           25.2812,  -20.4050,   86.7691,   85.9397,   13.6103,  -82.7592,\n",
       "          -85.0706,  -72.1738,   93.4526,  -65.7640,   71.9277,   15.3220,\n",
       "           98.3524,   22.2615,  -30.4144,   33.1140,  -36.4077,   66.7369,\n",
       "          -16.2854,   13.8943,   -2.7035,   98.8977,   90.1377,  -52.8782,\n",
       "          -76.5535,   20.3504,  -20.9450,   77.5493,   47.0417,   16.2594,\n",
       "         -101.7639,  -22.5705,   76.6870,  -76.1810,  -64.5362,  -64.9955,\n",
       "          -56.1546,   22.0030,   70.0952,   20.0590,  -96.2282,  -59.7047,\n",
       "           -8.4544,    7.7821,  -34.4538,   59.0451],\n",
       "        grad_fn=<NativeLayerNormBackwardBackward0>),\n",
       " tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
       "              0.0000,     0.0000,     0.0000,     0.0000],\n",
       "         [ -357.8239, -1015.5087,  -255.4738,  -132.9655,  -661.0648,  -620.5347,\n",
       "           -782.3305,  -333.4948,  -178.2808,  -586.4276,  -444.8063,  -540.8380,\n",
       "            131.1654,  -135.4654,   -33.7386,  -947.4818,  -442.2123,  -667.2500,\n",
       "           -205.5987,  -538.1839,  -170.0410,  -149.2140,   490.1361,   -16.6158,\n",
       "            227.4303,    99.2887,   208.6009,   893.7277,  -163.5655,  1516.5151,\n",
       "           -556.7501,   448.7328,  -166.2204,   393.2417,  -541.3427,   776.5844,\n",
       "           -465.9823,   334.1042,  -194.9146,   595.0859,  -779.1113,  1311.9724,\n",
       "           -614.7505,   723.7212,   182.5284,  1161.7733,    15.4252,   757.9580,\n",
       "           -193.5695,   157.0544,  -728.0889,   897.7690,  -280.9824,   600.7711,\n",
       "           -613.2711,   875.6465,  -436.0715,  1129.8540,  -232.3039,   906.5109,\n",
       "           -582.2330,   462.9093,  -582.8511,  1048.8524]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([  0.0000, 826.4346], grad_fn=<ViewBackward0>),\n",
       " tensor([[-6.7051e-06, -1.0191e-05,  7.7580e-06,  5.3934e-07, -5.7575e-06,\n",
       "          -2.8175e-06, -6.0029e-06,  5.3669e-06,  7.0394e-06,  4.6851e-06,\n",
       "          -1.0377e-05,  5.8817e-07,  2.2992e-06, -1.4136e-06, -3.2814e-07,\n",
       "           1.4092e-06, -6.7020e-06,  9.2760e-07, -5.6188e-06, -2.2700e-06,\n",
       "          -2.7499e-06, -4.8439e-06,  4.0595e-06, -1.5966e-06,  8.0923e-07,\n",
       "          -2.1833e-06,  2.0483e-06,  2.7520e-06, -1.0591e-06,  7.5670e-06,\n",
       "          -1.6073e-06,  3.7737e-06,  2.2574e-06,  3.0505e-06, -1.2083e-06,\n",
       "           1.6781e-06, -3.3071e-07,  8.3305e-07,  8.9883e-08,  1.7388e-06,\n",
       "          -4.1976e-06,  6.1112e-06, -2.7063e-06,  1.1631e-06,  1.5247e-06,\n",
       "           5.5730e-06,  7.2189e-08,  1.4969e-06,  2.0418e-07,  8.5851e-07,\n",
       "          -4.9751e-06,  2.1227e-06, -1.0197e-06,  3.6433e-06, -1.4470e-06,\n",
       "           2.2661e-06, -3.6277e-06,  5.1757e-06, -1.1813e-06,  3.7428e-06,\n",
       "          -2.2552e-06,  5.9197e-07, -2.5644e-06,  1.9196e-06]],\n",
       "        grad_fn=<TBackward0>),\n",
       " tensor([3.3159e-06], grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ln_P_d_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(psi_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "print((P := psi_x * psi_x.conj()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5769-6.1739e-01j, -0.6152+8.4863e-01j],\n",
       "        [ 1.7631+7.5909e-01j,  0.1033+1.3899e+00j],\n",
       "        [ 0.3123-2.5167e-03j,  0.0912-1.1067e+00j],\n",
       "        [ 0.9518-1.4823e+00j, -0.5423+4.7282e-01j],\n",
       "        [ 1.5811+1.1638e+00j,  0.5020+2.9848e+00j],\n",
       "        [-1.6522-1.4240e+00j, -0.2496-1.8722e-01j],\n",
       "        [-1.4371-4.0499e-01j,  1.7068+1.1778e+00j],\n",
       "        [-1.1488+1.8205e+00j, -0.1983-1.2034e+00j],\n",
       "        [ 0.2536+7.6231e-01j,  0.9571+1.1045e+00j],\n",
       "        [ 1.5606+4.7761e-01j,  0.1221-2.1698e-01j]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_re = torch.randn(10, 2)\n",
    "ex_im = torch.randn(10, 2)\n",
    "ex_com = torch.complex(ex_re, ex_im)\n",
    "ex_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.1365, 4.6943])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_com.norm(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape is psi_x * psi_x.conj()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 32])\n",
      "torch.Size([50, 32])\n"
     ]
    }
   ],
   "source": [
    "print(psi_x.shape)\n",
    "print((psi_x * psi_x.conj()).shape)  # This produces a probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3270, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, -2.3047,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.4629,  0.0000,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  5.6694,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.0000,  0.0000,  1.4428]]),)"
      ]
     },
     "execution_count": 640,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import vmap\n",
    "\n",
    "N = 5\n",
    "M = 12  # Batch dimension\n",
    "x = torch.randn(N, requires_grad=True)\n",
    "y = torch.randn(N, requires_grad=True)\n",
    "f = lambda x, y: x**2 + y**2\n",
    "z = f(x, y)\n",
    "print(z.shape)\n",
    "\n",
    "\n",
    "def get_vjp(v):\n",
    "    return torch.autograd.grad(z, x, grad_outputs=v, retain_graph=True)\n",
    "\n",
    "\n",
    "(jacobian := torch.vmap(get_vjp, randomness=\"same\")(torch.eye(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 12])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[641], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vjp\u001b[39m(v):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 14\u001b[0m (jacobian \u001b[38;5;241m:=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvmap(get_vjp, in_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, randomness\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)(torch\u001b[38;5;241m.\u001b[39meye(M)))\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    282\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[0;32mIn[641], line 11\u001b[0m, in \u001b[0;36mget_vjp\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vjp\u001b[39m(v):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _make_grads(\n\u001b[1;32m    386\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:82\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     80\u001b[0m     shape_matches \u001b[38;5;241m=\u001b[39m expect_true(sym_eq(out\u001b[38;5;241m.\u001b[39msize(), first_grad\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_matches:\n\u001b[0;32m---> 82\u001b[0m     out_shape, grad_shape \u001b[38;5;241m=\u001b[39m _calculate_shape(\n\u001b[1;32m     83\u001b[0m         out, first_grad, is_grads_batched\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:45\u001b[0m, in \u001b[0;36m_calculate_shape\u001b[0;34m(output, grad, is_grads_batched)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_shape\u001b[39m(\n\u001b[1;32m     41\u001b[0m     output: torch\u001b[38;5;241m.\u001b[39mTensor, grad: torch\u001b[38;5;241m.\u001b[39mTensor, is_grads_batched: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_ShapeorNestedShape, _ShapeorNestedShape]:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# is_same_size ensures that both tensors are either nested or non nested\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# circular import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NestedTensor\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, NestedTensor):\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:416\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ret_nt, offsets, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_contiguous \u001b[38;5;28;01melse\u001b[39;00m length_list)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# NB: A dummy arg is required so that NestedTensor.__torch_dispatch__() is invoked\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# for _nested_view_from_values_offsets(). Sizes don't matter much, but they shouldn't be\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# 0/1 because the dummy can be fake-ified and we want to avoid specializing.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# This arg is otherwise unused.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m _nt_view_dummy \u001b[38;5;241m=\u001b[39m NestedTensor(\n\u001b[1;32m    417\u001b[0m     values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    418\u001b[0m     offsets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    419\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnested_view_from_values_offsets\u001b[39m(values, offsets, ragged_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_nested_view_from_jagged(\n\u001b[1;32m    424\u001b[0m         values, offsets, _nt_view_dummy, \u001b[38;5;28;01mNone\u001b[39;00m, ragged_idx\n\u001b[1;32m    425\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:112\u001b[0m, in \u001b[0;36mNestedTensor.__init__\u001b[0;34m(self, values, offsets, lengths, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_cache \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# collapsed ragged dim must always be dynamic\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx)\n\u001b[1;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:223\u001b[0m, in \u001b[0;36mmark_dynamic\u001b[0;34m(t, index, min, max)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mMark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_traceable_wrapper_subclass(t):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# TODO: Make this configurable via a supported public API\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     _apply_func_to_inner_tensors_of_same_dim(\n\u001b[1;32m    224\u001b[0m         mark_dynamic, t, index, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dynamo_dynamic_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:178\u001b[0m, in \u001b[0;36m_apply_func_to_inner_tensors_of_same_dim\u001b[0;34m(func, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    177\u001b[0m     inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(t, attr)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inner\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39mdim():\n\u001b[1;32m    179\u001b[0m         func(inner, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:232\u001b[0m, in \u001b[0;36mNestedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:216\u001b[0m, in \u001b[0;36mNestedTensor.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m fn \u001b[38;5;241m=\u001b[39m lookup_jagged(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(func)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:181\u001b[0m, in \u001b[0;36mregister_func.<locals>.wrapper.<locals>.get_inner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 181\u001b[0m     check_schema(schema_str, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(aten_op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:118\u001b[0m, in \u001b[0;36mcheck_schema\u001b[0;34m(schema_str, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_fn(args[i]):\n\u001b[1;32m    110\u001b[0m     type_to_desc \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt?\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptional tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<any type>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     }\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNestedTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_to_desc[arg_type]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "M = 12  # Batch dimension\n",
    "x = torch.randn((N, M), requires_grad=True)\n",
    "y = torch.randn((N, M), requires_grad=True)\n",
    "f = lambda x, y: x**2 + y**2\n",
    "z = f(x, y)\n",
    "print(z.shape)\n",
    "\n",
    "\n",
    "def get_vjp(v):\n",
    "    return torch.autograd.grad(z, x, grad_outputs=v, retain_graph=True)\n",
    "\n",
    "\n",
    "(jacobian := torch.vmap(get_vjp, in_dims=1, randomness=\"same\")(torch.eye(M)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1966, -0.7046,  0.8144])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.dot has type [D], [D] -> []\n",
    "\n",
    "# What if we wanted to apply it across a batch dimension where the second dim is the batch dim? [N, D], [N, D] -> [D]\n",
    "\n",
    "batched_dot = vmap(torch.dot, in_dims=1)\n",
    "x, y = torch.randn(5, 3), torch.randn(5, 3)\n",
    "batched_dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to map torch.autograd.grad over a batch of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[648], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m grad_vmap \u001b[38;5;241m=\u001b[39m vmap(grad_on_seq, in_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, randomness\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m grad_vmap(torch\u001b[38;5;241m.\u001b[39meye(N))\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vmap_impl(func, in_dims, out_dims, randomness, chunk_size, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:281\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    278\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _flat_vmap(\n\u001b[1;32m    282\u001b[0m     func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    283\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:47\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_functorch/vmap.py:403\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    402\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 403\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39mbatched_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[0;32mIn[648], line 4\u001b[0m, in \u001b[0;36mgrad_on_seq\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_on_seq\u001b[39m(v):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(z, x, grad_outputs\u001b[38;5;241m=\u001b[39mv, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:385\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    378\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 385\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _make_grads(\n\u001b[1;32m    386\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[38;5;241m=\u001b[39mis_grads_batched\n\u001b[1;32m    387\u001b[0m )\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:82\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     80\u001b[0m     shape_matches \u001b[38;5;241m=\u001b[39m expect_true(sym_eq(out\u001b[38;5;241m.\u001b[39msize(), first_grad\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shape_matches:\n\u001b[0;32m---> 82\u001b[0m     out_shape, grad_shape \u001b[38;5;241m=\u001b[39m _calculate_shape(\n\u001b[1;32m     83\u001b[0m         out, first_grad, is_grads_batched\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/autograd/__init__.py:45\u001b[0m, in \u001b[0;36m_calculate_shape\u001b[0;34m(output, grad, is_grads_batched)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_shape\u001b[39m(\n\u001b[1;32m     41\u001b[0m     output: torch\u001b[38;5;241m.\u001b[39mTensor, grad: torch\u001b[38;5;241m.\u001b[39mTensor, is_grads_batched: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[_ShapeorNestedShape, _ShapeorNestedShape]:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# is_same_size ensures that both tensors are either nested or non nested\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# circular import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnested_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NestedTensor\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mis_nested \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, NestedTensor):\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_grads_batched:\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:416\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ret_nt, offsets, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m is_contiguous \u001b[38;5;28;01melse\u001b[39;00m length_list)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# NB: A dummy arg is required so that NestedTensor.__torch_dispatch__() is invoked\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# for _nested_view_from_values_offsets(). Sizes don't matter much, but they shouldn't be\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# 0/1 because the dummy can be fake-ified and we want to avoid specializing.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# This arg is otherwise unused.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m _nt_view_dummy \u001b[38;5;241m=\u001b[39m NestedTensor(\n\u001b[1;32m    417\u001b[0m     values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    418\u001b[0m     offsets\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m3\u001b[39m, (\u001b[38;5;241m2\u001b[39m,), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64),\n\u001b[1;32m    419\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnested_view_from_values_offsets\u001b[39m(values, offsets, ragged_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_nested_view_from_jagged(\n\u001b[1;32m    424\u001b[0m         values, offsets, _nt_view_dummy, \u001b[38;5;28;01mNone\u001b[39;00m, ragged_idx\n\u001b[1;32m    425\u001b[0m     )  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:112\u001b[0m, in \u001b[0;36mNestedTensor.__init__\u001b[0;34m(self, values, offsets, lengths, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata_cache \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_metadata_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# collapsed ragged dim must always be dynamic\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx)\n\u001b[1;32m    113\u001b[0m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ragged_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:223\u001b[0m, in \u001b[0;36mmark_dynamic\u001b[0;34m(t, index, min, max)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03mMark a tensor as having a dynamic dim and set corresponding min and max range for the dim.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_traceable_wrapper_subclass(t):\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# default behavior: mirror mark_dynamic() on all inner tensors with same dim as t\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# TODO: Make this configurable via a supported public API\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     _apply_func_to_inner_tensors_of_same_dim(\n\u001b[1;32m    224\u001b[0m         mark_dynamic, t, index, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dynamo_dynamic_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/_dynamo/decorators.py:178\u001b[0m, in \u001b[0;36m_apply_func_to_inner_tensors_of_same_dim\u001b[0;34m(func, t, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    177\u001b[0m     inner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(t, attr)\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inner\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39mdim():\n\u001b[1;32m    179\u001b[0m         func(inner, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:232\u001b[0m, in \u001b[0;36mNestedTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/nested_tensor.py:216\u001b[0m, in \u001b[0;36mNestedTensor.__torch_dispatch__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m fn \u001b[38;5;241m=\u001b[39m lookup_jagged(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(func)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:181\u001b[0m, in \u001b[0;36mregister_func.<locals>.wrapper.<locals>.get_inner.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 181\u001b[0m     check_schema(schema_str, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(aten_op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/tqs2/lib/python3.12/site-packages/torch/nested/_internal/ops.py:118\u001b[0m, in \u001b[0;36mcheck_schema\u001b[0;34m(schema_str, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_fn(args[i]):\n\u001b[1;32m    110\u001b[0m     type_to_desc \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt?\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptional tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<any type>\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     }\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNestedTensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_to_desc[arg_type]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: NestedTensor tensor_attr_supported_getter(self: jt_all): expected self to be a jagged layout NestedTensor"
     ]
    }
   ],
   "source": [
    "# A function that takes in a sample of dimension (N,) and returns the gradient with respect to\n",
    "# x\n",
    "def grad_on_seq(v):\n",
    "    return torch.autograd.grad(z, x, grad_outputs=v, retain_graph=True)\n",
    "\n",
    "\n",
    "grad_vmap = vmap(grad_on_seq, in_dims=1, randomness=\"same\")\n",
    "grad_vmap(torch.eye(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tqs2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
